{"cells":[{"cell_type":"markdown","metadata":{"id":"gzYw0Wh_2MiX"},"source":["# TP7: Fine-tuning BERT on Q&A tasks\n","\n","**Authors:** \n","- julien.denize@centralesupelec.fr\n","- tom.dupuis@centralesupelec.fr\n","\n","\n","If you have questions or suggestions, contact us and we will gladly answer and take into account your remarks.\n","\n","For this tp you need to have some ground understanding of pytorch. A basic introduction is available [here](https://pytorch.org/tutorials/beginner/basics/intro.html).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ufFFeRxvuQpb"},"source":["## Objective\n","\n","In this TP, we will implement solutions for the Question & Answering (Q&A) task by Fine-tuning a pretrained distilbert.\n","\n","This TP is built on the [HuggingFace Q&A tutorial](https://huggingface.co/docs/transformers/tasks/question_answering), therefore it relies on libraries associated to HuggingFace. \n","\n","Question answering tasks return an answer given a question. There are two common forms of question answering:\n","- Extractive: extract the answer from the given context.\n","- Abstractive: generate an answer from the context that correctly answers the question.\n","\n","In this TP, we will show you how to fine-tune [Distilbert](https://huggingface.co/docs/transformers/model_doc/distilbert) on the SQuAD dataset for extractive question answering.\n","\n","Distilbert is a smaller transformer architecture than BERT that has been trained by [knowledge distillation](https://en.wikipedia.org/wiki/Knowledge_distillation) of BERT to provide a lightweight faster NLP model with high performance.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WRUiEYYhwIBv"},"source":["## Your task\n","\n","Fill the missing parts in the code (parts between # --- START CODE HERE and # --- END CODE HERE)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"sKtndvKm2Gx0","executionInfo":{"status":"ok","timestamp":1666298416332,"user_tz":-120,"elapsed":726,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"}}},"outputs":[],"source":["import torch\n","import numpy as np\n","import random\n","\n","# Seed everything\n","seed=42\n","torch.manual_seed(seed)\n","random.seed(seed)\n","np.random.seed(seed)"]},{"cell_type":"markdown","metadata":{"id":"vwqqRns9XFOZ"},"source":["## Install the required libraries"]},{"cell_type":"markdown","metadata":{"id":"KBeMTVkRYpKe"},"source":["We need to install the following pip packages:\n","- [datasets](https://pypi.org/project/datasets/): to load datasets available on the [HuggingFace Datasets Hub](https://huggingface.co/datasets). \n","- [transformers](https://pypi.org/project/transformers/):  to load thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio in Pytorch, Tensorflow or JAX."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3549,"status":"ok","timestamp":1666298419874,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"},"user_tz":-120},"id":"Lcl5Lf-gwN2v","outputId":"af25676d-b4a2-4d1a-a796-de3bcc26ec60"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.6.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.23.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.1.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.10.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.13.0)\n","Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.8.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.9.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"]}],"source":["! pip install datasets transformers"]},{"cell_type":"markdown","metadata":{"id":"4aB3DYYlX1zC"},"source":["## Load the dataset"]},{"cell_type":"markdown","metadata":{"id":"HF37_b3hYZm7"},"source":["We will use the [SQUAD dataset](https://rajpurkar.github.io/SQuAD-explorer/).\n","\n",">Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n","\n","We will instantiate the train and validation splits via the [load_dataset](https://huggingface.co/docs/datasets/v1.11.0/splits.html) function.\n","\n","There are 87.599 elements in the train split and 10.570 elements in the validation split. We will only request 1% of each to avoid long training time. "]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":192,"referenced_widgets":["046230d261a64745accbf38b400f2a67","fe9c6020a6a74ad4aab243f644f04b49","0f54d1a500ba4bb5abac1a817e8df539","fd91c69345a64a2d9b82153204c7d8a3","464b34dacbf6413099b2dc3182bec6a9","7c705522c27e4febb57b404cba504a58","b71ffc739b37438cb5c3cbd00469126d","53e240692d0744e595d679342e1a8125","ee6ecee411b24f03a2777a8c1b16b59f","a6104cf999864ba2b42a531ac3a53d97","2b1cae93df604dd0a8a0871123543e0b"]},"executionInfo":{"elapsed":5966,"status":"ok","timestamp":1666298425830,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"},"user_tz":-120},"id":"07h1xn1LW8bV","outputId":"a041d1d5-b34c-4ec5-dba7-86c8ff453ef2"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.builder:Found cached dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"046230d261a64745accbf38b400f2a67"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["[Dataset({\n","     features: ['id', 'title', 'context', 'question', 'answers'],\n","     num_rows: 876\n"," }), Dataset({\n","     features: ['id', 'title', 'context', 'question', 'answers'],\n","     num_rows: 106\n"," })]"]},"metadata":{},"execution_count":3}],"source":["from datasets import load_dataset\n","\n","# --- START CODE HERE (01)\n","# Load the SQUAD dataset with 1% of each different splits.\n","dataset = load_dataset(\"squad\", split=['train[:1%]', 'validation[:1%]'])\n","\n","train_dataset = dataset[0]\n","validation_dataset = dataset[1]\n","# --- END CODE HERE\n","dataset"]},{"cell_type":"markdown","metadata":{"id":"Utd6TS2ZimtC"},"source":["Now we can have access to the samples in the datasets.\n","\n","In each sample we have the following information:\n","- the id of the wikipedia article.\n","- the title of the wikipedia article.\n","- the context that contains the answer to the question.\n","- the question.\n","- the answer along with the index of where the answer start."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":62,"status":"ok","timestamp":1666298425834,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"},"user_tz":-120},"id":"WkLPSav8i6Dl","outputId":"d158b6dc-4aad-4029-82ca-c44f51074f29"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'id': '5733be284776f41900661182',\n"," 'title': 'University_of_Notre_Dame',\n"," 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n"," 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n"," 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"]},"metadata":{},"execution_count":4}],"source":["train_dataset[0]"]},{"cell_type":"markdown","metadata":{"id":"QTv-Z_hwkFe3"},"source":["HuggingFace provides a nice function to better show what the data looks like."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"LAM6FB43jjV5","executionInfo":{"status":"ok","timestamp":1666298425837,"user_tz":-120,"elapsed":52,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"}}},"outputs":[],"source":["from datasets import ClassLabel, Sequence\n","import random\n","import pandas as pd\n","from IPython.display import display, HTML\n","\n","def show_random_elements(dataset, num_examples=10):\n","    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n","    picks = []\n","    for _ in range(num_examples):\n","        pick = random.randint(0, len(dataset)-1)\n","        while pick in picks:\n","            pick = random.randint(0, len(dataset)-1)\n","        picks.append(pick)\n","    \n","    df = pd.DataFrame(dataset[picks])\n","    for column, typ in dataset.features.items():\n","        if isinstance(typ, ClassLabel):\n","            df[column] = df[column].transform(lambda i: typ.names[i])\n","        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n","            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n","    display(HTML(df.to_html()))"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":387},"executionInfo":{"elapsed":49,"status":"ok","timestamp":1666298425837,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"},"user_tz":-120},"id":"YT3MOBm3kJtP","outputId":"4e8bfed0-adb6-4d14-d80e-2fbdbca972c4"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>title</th>\n","      <th>context</th>\n","      <th>question</th>\n","      <th>answers</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>56beb0683aeaaa14008c9213</td>\n","      <td>Beyoncé</td>\n","      <td>In 2015 Beyoncé signed an open letter which the ONE Campaign had been collecting signatures for; the letter was addressed to Angela Merkel and Nkosazana Dlamini-Zuma, urging them to focus on women as they serve as the head of the G7 in Germany and the AU in South Africa respectively, which will start to set the priorities in development funding before a main UN summit in September 2015 that will establish new development goals for the generation.</td>\n","      <td>An important UN summit took place when?</td>\n","      <td>{'text': ['September 2015'], 'answer_start': [374]}</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5733926d4776f41900660d8f</td>\n","      <td>University_of_Notre_Dame</td>\n","      <td>The Rev. John J. Cavanaugh, C.S.C. served as president from 1946 to 1952. Cavanaugh's legacy at Notre Dame in the post-war years was devoted to raising academic standards and reshaping the university administration to suit it to an enlarged educational mission and an expanded student body and stressing advanced studies and research at a time when Notre Dame quadrupled in student census, undergraduate enrollment increased by more than half, and graduate student enrollment grew fivefold. Cavanaugh also established the Lobund Institute for Animal Studies and Notre Dame's Medieval Institute. Cavanaugh also presided over the construction of the Nieuwland Science Hall, Fisher Hall, and the Morris Inn, as well as the Hall of Liberal Arts (now O'Shaughnessy Hall), made possible by a donation from I.A. O'Shaughnessy, at the time the largest ever made to an American Catholic university. Cavanaugh also established a system of advisory councils at the university, which continue today and are vital to the university's governance and development</td>\n","      <td>Which institute involving animal life did Cavanaugh create at Notre Dame?</td>\n","      <td>{'text': ['Lobund Institute for Animal Studies'], 'answer_start': [522]}</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5733a7bd4776f41900660f6c</td>\n","      <td>University_of_Notre_Dame</td>\n","      <td>The university first offered graduate degrees, in the form of a Master of Arts (MA), in the 1854–1855 academic year. The program expanded to include Master of Laws (LL.M.) and Master of Civil Engineering in its early stages of growth, before a formal graduate school education was developed with a thesis not required to receive the degrees. This changed in 1924 with formal requirements developed for graduate degrees, including offering Doctorate (PhD) degrees. Today each of the five colleges offer graduate education. Most of the departments from the College of Arts and Letters offer PhD programs, while a professional Master of Divinity (M.Div.) program also exists. All of the departments in the College of Science offer PhD programs, except for the Department of Pre-Professional Studies. The School of Architecture offers a Master of Architecture, while each of the departments of the College of Engineering offer PhD programs. The College of Business offers multiple professional programs including MBA and Master of Science in Accountancy programs. It also operates facilities in Chicago and Cincinnati for its executive MBA program. Additionally, the Alliance for Catholic Education program offers a Master of Education program where students study at the university during the summer and teach in Catholic elementary schools, middle schools, and high schools across the Southern United States for two school years.</td>\n","      <td>What type of degree is an M.Div.?</td>\n","      <td>{'text': ['Master of Divinity'], 'answer_start': [624]}</td>\n","    </tr>\n","  </tbody>\n","</table>"]},"metadata":{}}],"source":["show_random_elements(train_dataset, 3)"]},{"cell_type":"markdown","metadata":{"id":"VvUhN8m7kW2p"},"source":["## Preprocess the training data\n"]},{"cell_type":"markdown","metadata":{"id":"es8h9C7pYDho"},"source":["\n","Now that we have access to the data, we need to preprocess it to feed it to our neural networks.\n","\n","In NLP, this step consist of making the tokenization of the data, meaning convert the string words into unique IDs.\n","\n","Our model requires the following as input:\n","- A first sequence that is the question.\n","- A separator token [SEP].\n","- A second sequence that may contain the answer.\n","\n","The label is given by the start and end indices of the tokens that compose the answer.\n","\n","![](https://miro.medium.com/max/1400/1*QhIXsDBEnANLXMA0yONxxA.png)"]},{"cell_type":"markdown","metadata":{"id":"NIReoeO1siVU"},"source":["### Instantiate the tokenizer\n","\n","We will use the [AutoTokenizer](https://huggingface.co/docs/transformers/model_doc/auto) class provided by HuggingFace as this will ensure we use the tokenizer that was used to train the distilbert model. For that, we need to use the right checkpoint of the model. The list of checkpoints is available [here](https://huggingface.co/models) and you need to retrieve the basic checkpoint for distilbert that do not care about the case (ENGLISH = english). "]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":85,"referenced_widgets":["a06763d3c344458084fa5f5b5f3e8c4b","e03463bf84f946c1b727a3bcbd814d77","0cbac09bea6243988a52444250349820","e5ee9fbe20e04648b18fc4028da8f7be","19dd94bd50a241baaa96e9db2a473641","6b18fb29df864c768aa813c186540d8c","c17098156ca64c2b947843d12b172a8c","d54c41fbf13a477baa8897a6b514a097","af3699612ed54d84950757ec28db226c","d6e68e9608454b4fb395f14fc475ba1e","6cd6e76609d2484ba2bc329efb7342f5"]},"executionInfo":{"elapsed":589,"status":"ok","timestamp":1666298426381,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"},"user_tz":-120},"id":"EtR4hIIYp3v1","outputId":"287992bf-b74e-4ff5-aa7b-f55e66c059e6"},"outputs":[{"output_type":"stream","name":"stderr","text":["The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"]},{"output_type":"stream","name":"stdout","text":["Moving 0 files to the new cache system\n"]},{"output_type":"display_data","data":{"text/plain":["0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a06763d3c344458084fa5f5b5f3e8c4b"}},"metadata":{}}],"source":["# --- START CODE HERE (02)\n","# Import the auto tokenizer class and instantiate it\n","from transformers import AutoTokenizer\n","    \n","model_checkpoint = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","# --- END CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"jJiwbTw_py8u"},"source":["You can try the tokenizer with custom strings or from our data. Tokenizer accepts tuple as input, but returns only one concatenated tokenized output with a separator token [SEP] with a starting token [CLS]."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1666298426382,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"},"user_tz":-120},"id":"dftzIeKfrcrf","outputId":"0076af57-066c-4bac-da63-8c7163b4deba"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [101, 7632, 1010, 1045, 2052, 2293, 2000, 3231, 1996, 19204, 17629, 1012, 102, 2469, 1010, 2175, 3805, 1998, 20410, 2008, 2394, 1027, 2394, 1027, 2394, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"metadata":{},"execution_count":8}],"source":["custom_string = \"Hi, I would love to test the tokenizer.\"\n","custom_string_2 = \"Sure, go ahead and verify that english = ENGLISH = ENglISh.\"\n","tokenized_custom_strings = tokenizer(custom_string, custom_string_2)\n","tokenized_custom_strings"]},{"cell_type":"markdown","metadata":{"id":"NbQZg1GHI2rW"},"source":["You can now decode the sequence and retrieve the initial string with the special tokens. You can also verify that the case is no longer present in the string as our model do not make the difference between lower and upper case. "]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":2393,"status":"ok","timestamp":1666298428770,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"},"user_tz":-120},"id":"bGp30TPvwq8s","outputId":"86161155-a126-4811-f3ea-e56e8f7dc2bd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'[CLS] hi, i would love to test the tokenizer. [SEP] sure, go ahead and verify that english = english = english. [SEP]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}],"source":["tokenizer.decode(tokenized_custom_strings[\"input_ids\"])"]},{"cell_type":"markdown","metadata":{"id":"mT0leg9kJImx"},"source":["Here we can apply the tokenizer on one question from our training set."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":568,"status":"ok","timestamp":1666298429310,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"},"user_tz":-120},"id":"dKQnsoeYrjei","outputId":"261cab3c-ca5c-444b-9bee-b3dfa14ad890"},"outputs":[{"output_type":"stream","name":"stdout","text":["To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n","{'input_ids': [101, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","[CLS] to whom did the virgin mary allegedly appear in 1858 in lourdes france? [SEP]\n"]}],"source":["print(train_dataset[0][\"question\"])\n","print(tokenizer(train_dataset[0][\"question\"]))\n","print(tokenizer.decode(tokenizer(train_dataset[0][\"question\"])[\"input_ids\"]))"]},{"cell_type":"markdown","metadata":{"id":"Vk-jB-BeslM0"},"source":["### Deal with long contexts\n","\n","Our model can only take a maximum number of tokens per input. Our input is composed of both the question and the context separated by the special token [SEP]. \n","\n","However, in our dataset we might have some samples where the question plus the context length is larger than this maximum number of tokens. We cannot just truncate the input as for some other tasks as the answer to the question might be located in the cut part. \n","\n","Instead, a long context will be splitted in several input features, each of length shorter than the maximum length of the model. To avoid that the answer is located on the splitting point, we will make the input features overlap."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"6SPNp_LHsML_","executionInfo":{"status":"ok","timestamp":1666298429311,"user_tz":-120,"elapsed":55,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"}}},"outputs":[],"source":["max_length = 384 # The maximum length of a feature (question and context)\n","doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed."]},{"cell_type":"markdown","metadata":{"id":"JMoDoYqPvk_z"},"source":["Below is the code to find the first example with a long input:"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":54,"status":"ok","timestamp":1666298429311,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"},"user_tz":-120},"id":"whWe6iccvkiA","outputId":"3d2be83a-6c15-4325-c3fd-7cbfca9e855e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["({'id': '5733caf74776f4190066124c',\n","  'title': 'University_of_Notre_Dame',\n","  'context': \"The men's basketball team has over 1,600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 NCAA tournaments. Former player Austin Carr holds the record for most points scored in a single game of the tournament with 61. Although the team has never won the NCAA Tournament, they were named by the Helms Athletic Foundation as national champions twice. The team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending UCLA's record 88-game winning streak in 1974. The team has beaten an additional eight number-one teams, and those nine wins rank second, to UCLA's 10, all-time in wins against the top team. The team plays in newly renovated Purcell Pavilion (within the Edmund P. Joyce Center), which reopened for the beginning of the 2009–2010 season. The team is coached by Mike Brey, who, as of the 2014–15 season, his fifteenth at Notre Dame, has achieved a 332-165 record. In 2009 they were invited to the NIT, where they advanced to the semifinals but were beaten by Penn State who went on and beat Baylor in the championship. The 2010–11 team concluded its regular season ranked number seven in the country, with a record of 25–5, Brey's fifth straight 20-win season, and a second-place finish in the Big East. During the 2014-15 season, the team went 32-6 and won the ACC conference tournament, later advancing to the Elite 8, where the Fighting Irish lost on a missed buzzer-beater against then undefeated Kentucky. Led by NBA draft picks Jerian Grant and Pat Connaughton, the Fighting Irish beat the eventual national champion Duke Blue Devils twice during the season. The 32 wins were the most by the Fighting Irish team since 1908-09.\",\n","  'question': \"How many wins does the Notre Dame men's basketball team have?\",\n","  'answers': {'text': ['over 1,600'], 'answer_start': [30]}},\n"," 395)"]},"metadata":{},"execution_count":12}],"source":["for i, example in enumerate(train_dataset):\n","    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > max_length:\n","        long_context_idx = i\n","        break\n","long_example = train_dataset[long_context_idx]\n","long_example, len(tokenizer(long_example[\"question\"] + long_example[\"context\"])[\"input_ids\"])"]},{"cell_type":"markdown","metadata":{"id":"OVqBqbFlvikK"},"source":["To split the input in several features, we need to correctly configure our tokenizer and feed it with inputs following these requirements:\n","- Pass to the tokenizer the tuple of the question and the context. It will automatically add the [SEP] token between the two.\n","- Force the tokenizer to split the input if it is too large:\n","  - only the second part (the context) can be truncated so that the question is shared by all new inputs.\n","  - allow overlapping between tokens.\n","\n","All these requirements can be done thanks to the [tokenizer utilities](https://huggingface.co/docs/transformers/v4.23.1/en/internal/tokenization_utils) by applying the correct parameters."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47,"status":"ok","timestamp":1666298429312,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"},"user_tz":-120},"id":"8ixyM9XXyVvN","outputId":"872b9e82-67b5-4017-c0ca-650c4b725de4"},"outputs":[{"output_type":"stream","name":"stdout","text":["The long example now has 2 inputs with length [384, 157].\n","[CLS] how many wins does the notre dame men's basketball team have? [SEP] the men's basketball team has over 1, 600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 ncaa tournaments. former player austin carr holds the record for most points scored in a single game of the tournament with 61. although the team has never won the ncaa tournament, they were named by the helms athletic foundation as national champions twice. the team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending ucla's record 88 - game winning streak in 1974. the team has beaten an additional eight number - one teams, and those nine wins rank second, to ucla's 10, all - time in wins against the top team. the team plays in newly renovated purcell pavilion ( within the edmund p. joyce center ), which reopened for the beginning of the 2009 – 2010 season. the team is coached by mike brey, who, as of the 2014 – 15 season, his fifteenth at notre dame, has achieved a 332 - 165 record. in 2009 they were invited to the nit, where they advanced to the semifinals but were beaten by penn state who went on and beat baylor in the championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were [SEP]\n","[CLS] how many wins does the notre dame men's basketball team have? [SEP] championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were the most by the fighting irish team since 1908 - 09. [SEP]\n"]}],"source":["# --- START CODE HERE (03)\n","# Tokenize the long example with the requirements defined above.\n","tokenized_long_example = tokenizer(\n","    long_example[\"question\"],\n","    long_example[\"context\"],\n","    max_length=max_length,\n","    truncation=\"only_second\",\n","    return_overflowing_tokens=True,\n","    stride=doc_stride\n",")\n","# --- END CODE HERE\n","\n","print(f\"The long example now has {len(tokenized_long_example['input_ids'])} inputs with length {[len(x) for x in tokenized_long_example['input_ids']]}.\") # Should have 2 inputs with length [384, 157]\n","for sequence in tokenized_long_example['input_ids']:\n","  print(tokenizer.decode(sequence))"]},{"cell_type":"markdown","metadata":{"id":"ELPkQLQp4t1W"},"source":["The problem with the above solution is that we lack the information of where is located the answer: we need to know where is located the answer for each feature provided. \n","\n","The model require the start and end positions of the answers in the tokens, so we will also need to map parts of the original context to some tokens.\n","\n","We need for each index of our feature the corresponding start and end character in the original text that gave our token in the format (`start_char`, `end_char`). The first token (`[CLS]`) has (0, 0) because it is a special added token that was not present in the original sentence.\n","\n","This can be done using the tokenizer utilities."]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1666298429313,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"},"user_tz":-120},"id":"D3xJF4kMzgZL","outputId":"3a2dbbea-ca99-44ae-88f9-9512ec46042f"},"outputs":[{"output_type":"stream","name":"stdout","text":["The long example now has 2 inputs with length [384, 157].\n","\n","\n","[CLS] how many wins does the notre dame men's basketball team have? [SEP] the men's basketball team has over 1, 600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 ncaa tournaments. former player austin carr holds the record for most points scored in a single game of the tournament with 61. although the team has never won the ncaa tournament, they were named by the helms athletic foundation as national champions twice. the team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending ucla's record 88 - game winning streak in 1974. the team has beaten an additional eight number - one teams, and those nine wins rank second, to ucla's 10, all - time in wins against the top team. the team plays in newly renovated purcell pavilion ( within the edmund p. joyce center ), which reopened for the beginning of the 2009 – 2010 season. the team is coached by mike brey, who, as of the 2014 – 15 season, his fifteenth at notre dame, has achieved a 332 - 165 record. in 2009 they were invited to the nit, where they advanced to the semifinals but were beaten by penn state who went on and beat baylor in the championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were [SEP]\n","[101, 2129, 2116, 5222, 2515, 1996, 10289, 8214, 2273, 1005, 1055, 3455, 2136, 2031, 1029, 102, 1996, 2273, 1005, 1055, 3455, 2136, 2038, 2058, 1015, 1010, 5174, 5222, 1010, 2028, 1997, 2069, 2260, 2816, 2040, 2031, 2584, 2008, 2928, 1010, 1998, 2031, 2596, 1999, 2654, 5803, 8504, 1012, 2280, 2447, 5899, 12385, 4324, 1996, 2501, 2005, 2087, 2685, 3195, 1999, 1037, 2309, 2208, 1997, 1996, 2977, 2007, 6079, 1012, 2348, 1996, 2136, 2038, 2196, 2180, 1996, 5803, 2977, 1010, 2027, 2020, 2315, 2011, 1996, 16254, 2015, 5188, 3192, 2004, 2120, 3966, 3807, 1012, 1996, 2136, 2038, 23339, 1037, 2193, 1997, 6314, 2015, 1997, 2193, 2028, 4396, 2780, 1010, 1996, 2087, 3862, 1997, 2029, 2001, 4566, 12389, 1005, 1055, 2501, 6070, 1011, 2208, 3045, 9039, 1999, 3326, 1012, 1996, 2136, 2038, 7854, 2019, 3176, 2809, 2193, 1011, 2028, 2780, 1010, 1998, 2216, 3157, 5222, 4635, 2117, 1010, 2000, 12389, 1005, 1055, 2184, 1010, 2035, 1011, 2051, 1999, 5222, 2114, 1996, 2327, 2136, 1012, 1996, 2136, 3248, 1999, 4397, 10601, 26429, 10531, 1006, 2306, 1996, 9493, 1052, 1012, 11830, 2415, 1007, 1010, 2029, 11882, 2005, 1996, 2927, 1997, 1996, 2268, 1516, 2230, 2161, 1012, 1996, 2136, 2003, 8868, 2011, 3505, 7987, 3240, 1010, 2040, 1010, 2004, 1997, 1996, 2297, 1516, 2321, 2161, 1010, 2010, 16249, 2012, 10289, 8214, 1010, 2038, 4719, 1037, 29327, 1011, 13913, 2501, 1012, 1999, 2268, 2027, 2020, 4778, 2000, 1996, 9152, 2102, 1010, 2073, 2027, 3935, 2000, 1996, 8565, 2021, 2020, 7854, 2011, 9502, 2110, 2040, 2253, 2006, 1998, 3786, 23950, 1999, 1996, 2528, 1012, 1996, 2230, 1516, 2340, 2136, 5531, 2049, 3180, 2161, 4396, 2193, 2698, 1999, 1996, 2406, 1010, 2007, 1037, 2501, 1997, 2423, 1516, 1019, 1010, 7987, 3240, 1005, 1055, 3587, 3442, 2322, 1011, 2663, 2161, 1010, 1998, 1037, 2117, 1011, 2173, 3926, 1999, 1996, 2502, 2264, 1012, 2076, 1996, 2297, 1011, 2321, 2161, 1010, 1996, 2136, 2253, 3590, 1011, 1020, 1998, 2180, 1996, 16222, 3034, 2977, 1010, 2101, 10787, 2000, 1996, 7069, 1022, 1010, 2073, 1996, 3554, 3493, 2439, 2006, 1037, 4771, 12610, 2121, 1011, 3786, 2121, 2114, 2059, 15188, 5612, 1012, 2419, 2011, 6452, 4433, 11214, 15333, 6862, 3946, 1998, 6986, 9530, 2532, 18533, 2239, 1010, 1996, 3554, 3493, 3786, 1996, 9523, 2120, 3410, 3804, 2630, 13664, 3807, 2076, 1996, 2161, 1012, 1996, 3590, 5222, 2020, 102]\n","[(0, 0), (0, 3), (4, 8), (9, 13), (14, 18), (19, 22), (23, 28), (29, 33), (34, 37), (37, 38), (38, 39), (40, 50), (51, 55), (56, 60), (60, 61), (0, 0), (0, 3), (4, 7), (7, 8), (8, 9), (10, 20), (21, 25), (26, 29), (30, 34), (35, 36), (36, 37), (37, 40), (41, 45), (45, 46), (47, 50), (51, 53), (54, 58), (59, 61), (62, 69), (70, 73), (74, 78), (79, 86), (87, 91), (92, 96), (96, 97), (98, 101), (102, 106), (107, 115), (116, 118), (119, 121), (122, 126), (127, 138), (138, 139), (140, 146), (147, 153), (154, 160), (161, 165), (166, 171), (172, 175), (176, 182), (183, 186), (187, 191), (192, 198), (199, 205), (206, 208), (209, 210), (211, 217), (218, 222), (223, 225), (226, 229), (230, 240), (241, 245), (246, 248), (248, 249), (250, 258), (259, 262), (263, 267), (268, 271), (272, 277), (278, 281), (282, 285), (286, 290), (291, 301), (301, 302), (303, 307), (308, 312), (313, 318), (319, 321), (322, 325), (326, 330), (330, 331), (332, 340), (341, 351), (352, 354), (355, 363), (364, 373), (374, 379), (379, 380), (381, 384), (385, 389), (390, 393), (394, 406), (407, 408), (409, 415), (416, 418), (419, 424), (424, 425), (426, 428), (429, 435), (436, 439), (440, 446), (447, 452), (452, 453), (454, 457), (458, 462), (463, 470), (471, 473), (474, 479), (480, 483), (484, 490), (491, 495), (495, 496), (496, 497), (498, 504), (505, 507), (507, 508), (508, 512), (513, 520), (521, 527), (528, 530), (531, 535), (535, 536), (537, 540), (541, 545), (546, 549), (550, 556), (557, 559), (560, 570), (571, 576), (577, 583), (583, 584), (584, 587), (588, 593), (593, 594), (595, 598), (599, 604), (605, 609), (610, 614), (615, 619), (620, 626), (626, 627), (628, 630), (631, 635), (635, 636), (636, 637), (638, 640), (640, 641), (642, 645), (645, 646), (646, 650), (651, 653), (654, 658), (659, 666), (667, 670), (671, 674), (675, 679), (679, 680), (681, 684), (685, 689), (690, 695), (696, 698), (699, 704), (705, 714), (715, 722), (723, 731), (732, 733), (733, 739), (740, 743), (744, 750), (751, 752), (752, 753), (754, 759), (760, 766), (766, 767), (767, 768), (769, 774), (775, 783), (784, 787), (788, 791), (792, 801), (802, 804), (805, 808), (809, 813), (813, 814), (814, 818), (819, 825), (825, 826), (827, 830), (831, 835), (836, 838), (839, 846), (847, 849), (850, 854), (855, 857), (857, 859), (859, 860), (861, 864), (864, 865), (866, 868), (869, 871), (872, 875), (876, 880), (880, 881), (881, 883), (884, 890), (890, 891), (892, 895), (896, 905), (906, 908), (909, 914), (915, 919), (919, 920), (921, 924), (925, 933), (934, 935), (936, 939), (939, 940), (940, 943), (944, 950), (950, 951), (952, 954), (955, 959), (960, 964), (965, 969), (970, 977), (978, 980), (981, 984), (985, 987), (987, 988), (988, 989), (990, 995), (996, 1000), (1001, 1009), (1010, 1012), (1013, 1016), (1017, 1027), (1028, 1031), (1032, 1036), (1037, 1043), (1044, 1046), (1047, 1051), (1052, 1057), (1058, 1061), (1062, 1066), (1067, 1069), (1070, 1073), (1074, 1078), (1079, 1085), (1086, 1088), (1089, 1092), (1093, 1105), (1105, 1106), (1107, 1110), (1111, 1115), (1115, 1116), (1116, 1118), (1119, 1123), (1124, 1133), (1134, 1137), (1138, 1145), (1146, 1152), (1153, 1159), (1160, 1166), (1167, 1172), (1173, 1175), (1176, 1179), (1180, 1187), (1187, 1188), (1189, 1193), (1194, 1195), (1196, 1202), (1203, 1205), (1206, 1208), (1208, 1209), (1209, 1210), (1210, 1211), (1212, 1214), (1214, 1216), (1216, 1217), (1217, 1218), (1219, 1224), (1225, 1233), (1234, 1236), (1236, 1237), (1237, 1240), (1241, 1247), (1247, 1248), (1249, 1252), (1253, 1254), (1255, 1261), (1261, 1262), (1262, 1267), (1268, 1274), (1275, 1277), (1278, 1281), (1282, 1285), (1286, 1290), (1290, 1291), (1292, 1298), (1299, 1302), (1303, 1307), (1307, 1308), (1308, 1310), (1311, 1317), (1317, 1318), (1319, 1322), (1323, 1327), (1328, 1332), (1333, 1335), (1335, 1336), (1336, 1337), (1338, 1341), (1342, 1345), (1346, 1349), (1350, 1353), (1354, 1364), (1365, 1375), (1375, 1376), (1377, 1382), (1383, 1392), (1393, 1395), (1396, 1399), (1400, 1405), (1406, 1407), (1407, 1408), (1409, 1414), (1415, 1418), (1419, 1427), (1428, 1433), (1434, 1438), (1439, 1441), (1442, 1443), (1444, 1450), (1451, 1455), (1455, 1457), (1457, 1458), (1458, 1462), (1462, 1464), (1465, 1472), (1473, 1477), (1478, 1488), (1489, 1497), (1497, 1498), (1499, 1502), (1503, 1505), (1506, 1509), (1510, 1515), (1516, 1521), (1522, 1524), (1524, 1528), (1529, 1534), (1535, 1538), (1539, 1542), (1543, 1546), (1546, 1548), (1548, 1552), (1552, 1554), (1554, 1555), (1556, 1559), (1560, 1568), (1569, 1574), (1575, 1579), (1580, 1583), (1584, 1592), (1593, 1601), (1602, 1610), (1611, 1615), (1616, 1620), (1621, 1627), (1628, 1633), (1634, 1640), (1641, 1644), (1645, 1651), (1651, 1652), (1653, 1656), (1657, 1659), (1660, 1664), (1665, 1669), (0, 0)]\n","\n","\n","[CLS] how many wins does the notre dame men's basketball team have? [SEP] championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were the most by the fighting irish team since 1908 - 09. [SEP]\n","[101, 2129, 2116, 5222, 2515, 1996, 10289, 8214, 2273, 1005, 1055, 3455, 2136, 2031, 1029, 102, 2528, 1012, 1996, 2230, 1516, 2340, 2136, 5531, 2049, 3180, 2161, 4396, 2193, 2698, 1999, 1996, 2406, 1010, 2007, 1037, 2501, 1997, 2423, 1516, 1019, 1010, 7987, 3240, 1005, 1055, 3587, 3442, 2322, 1011, 2663, 2161, 1010, 1998, 1037, 2117, 1011, 2173, 3926, 1999, 1996, 2502, 2264, 1012, 2076, 1996, 2297, 1011, 2321, 2161, 1010, 1996, 2136, 2253, 3590, 1011, 1020, 1998, 2180, 1996, 16222, 3034, 2977, 1010, 2101, 10787, 2000, 1996, 7069, 1022, 1010, 2073, 1996, 3554, 3493, 2439, 2006, 1037, 4771, 12610, 2121, 1011, 3786, 2121, 2114, 2059, 15188, 5612, 1012, 2419, 2011, 6452, 4433, 11214, 15333, 6862, 3946, 1998, 6986, 9530, 2532, 18533, 2239, 1010, 1996, 3554, 3493, 3786, 1996, 9523, 2120, 3410, 3804, 2630, 13664, 3807, 2076, 1996, 2161, 1012, 1996, 3590, 5222, 2020, 1996, 2087, 2011, 1996, 3554, 3493, 2136, 2144, 5316, 1011, 5641, 1012, 102]\n","[(0, 0), (0, 3), (4, 8), (9, 13), (14, 18), (19, 22), (23, 28), (29, 33), (34, 37), (37, 38), (38, 39), (40, 50), (51, 55), (56, 60), (60, 61), (0, 0), (1093, 1105), (1105, 1106), (1107, 1110), (1111, 1115), (1115, 1116), (1116, 1118), (1119, 1123), (1124, 1133), (1134, 1137), (1138, 1145), (1146, 1152), (1153, 1159), (1160, 1166), (1167, 1172), (1173, 1175), (1176, 1179), (1180, 1187), (1187, 1188), (1189, 1193), (1194, 1195), (1196, 1202), (1203, 1205), (1206, 1208), (1208, 1209), (1209, 1210), (1210, 1211), (1212, 1214), (1214, 1216), (1216, 1217), (1217, 1218), (1219, 1224), (1225, 1233), (1234, 1236), (1236, 1237), (1237, 1240), (1241, 1247), (1247, 1248), (1249, 1252), (1253, 1254), (1255, 1261), (1261, 1262), (1262, 1267), (1268, 1274), (1275, 1277), (1278, 1281), (1282, 1285), (1286, 1290), (1290, 1291), (1292, 1298), (1299, 1302), (1303, 1307), (1307, 1308), (1308, 1310), (1311, 1317), (1317, 1318), (1319, 1322), (1323, 1327), (1328, 1332), (1333, 1335), (1335, 1336), (1336, 1337), (1338, 1341), (1342, 1345), (1346, 1349), (1350, 1353), (1354, 1364), (1365, 1375), (1375, 1376), (1377, 1382), (1383, 1392), (1393, 1395), (1396, 1399), (1400, 1405), (1406, 1407), (1407, 1408), (1409, 1414), (1415, 1418), (1419, 1427), (1428, 1433), (1434, 1438), (1439, 1441), (1442, 1443), (1444, 1450), (1451, 1455), (1455, 1457), (1457, 1458), (1458, 1462), (1462, 1464), (1465, 1472), (1473, 1477), (1478, 1488), (1489, 1497), (1497, 1498), (1499, 1502), (1503, 1505), (1506, 1509), (1510, 1515), (1516, 1521), (1522, 1524), (1524, 1528), (1529, 1534), (1535, 1538), (1539, 1542), (1543, 1546), (1546, 1548), (1548, 1552), (1552, 1554), (1554, 1555), (1556, 1559), (1560, 1568), (1569, 1574), (1575, 1579), (1580, 1583), (1584, 1592), (1593, 1601), (1602, 1610), (1611, 1615), (1616, 1620), (1621, 1627), (1628, 1633), (1634, 1640), (1641, 1644), (1645, 1651), (1651, 1652), (1653, 1656), (1657, 1659), (1660, 1664), (1665, 1669), (1670, 1673), (1674, 1678), (1679, 1681), (1682, 1685), (1686, 1694), (1695, 1700), (1701, 1705), (1706, 1711), (1712, 1716), (1716, 1717), (1717, 1719), (1719, 1720), (0, 0)]\n"]}],"source":["# --- START CODE HERE (04)\n","# Tokenize the long example with the new requirement defined above.\n","tokenized_long_example = tokenizer(\n","    example[\"question\"],\n","    example[\"context\"],\n","    max_length=max_length,\n","    truncation=\"only_second\",\n","    return_overflowing_tokens=True,\n","    return_offsets_mapping=True,\n","    stride=doc_stride\n",")\n","# --- END CODE HERE\n","\n","print(f\"The long example now has {len(tokenized_long_example['input_ids'])} inputs with length {[len(x) for x in tokenized_long_example['input_ids']]}.\") # Should have 2 inputs with length [384, 157]\n","for sequence, mapping in zip(tokenized_long_example['input_ids'], tokenized_long_example[\"offset_mapping\"]):\n","  print(\"\\n\")\n","  print(tokenizer.decode(sequence))\n","  print(sequence)\n","  print(mapping)"]},{"cell_type":"markdown","metadata":{"id":"Pyg8Uppx-wBW"},"source":["The mapping can be used to find the position of the start and end tokens of our answer in a feature. To avoid the question part we can use the `sequence_ids` field provided by the tokenizer output to have the knowledge of which tokens are part of the first sequence (the question) or the second sequence (the context, or part of the context).\n","\n","It returns for each token, the sequence ID (0 for question, 1 for context) and None for special tokens."]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1666298429313,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"},"user_tz":-120},"id":"k5ZC9JbUzipM","outputId":"f8fe4109-a1e5-4d2b-f171-2844d7601e55"},"outputs":[{"output_type":"stream","name":"stdout","text":["[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"]}],"source":["sequence_ids = tokenized_long_example.sequence_ids()\n","print(sequence_ids)"]},{"cell_type":"markdown","metadata":{"id":"mGOjgQeG_kAB"},"source":["Now, we can retrieve the answer from our features."]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1666298429314,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"},"user_tz":-120},"id":"GmlFl0ut_mhU","outputId":"9e393fb9-0a66-42a1-f11d-e9012dd3a87c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking for the answer `over 1,600` to the question `How many wins does the Notre Dame men's basketball team have?` in feature 1.\n","The feature contains the following decoded sequence:\n","[CLS] how many wins does the notre dame men's basketball team have? [SEP] the men's basketball team has over 1, 600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 ncaa tournaments. former player austin carr holds the record for most points scored in a single game of the tournament with 61. although the team has never won the ncaa tournament, they were named by the helms athletic foundation as national champions twice. the team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending ucla's record 88 - game winning streak in 1974. the team has beaten an additional eight number - one teams, and those nine wins rank second, to ucla's 10, all - time in wins against the top team. the team plays in newly renovated purcell pavilion ( within the edmund p. joyce center ), which reopened for the beginning of the 2009 – 2010 season. the team is coached by mike brey, who, as of the 2014 – 15 season, his fifteenth at notre dame, has achieved a 332 - 165 record. in 2009 they were invited to the nit, where they advanced to the semifinals but were beaten by penn state who went on and beat baylor in the championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were [SEP]\n","Answer found by the tokenizer at the token positions: 23, 26\n","over 1, 600\n","\n","\n","Looking for the answer `over 1,600` to the question `How many wins does the Notre Dame men's basketball team have?` in feature 2.\n","The feature contains the following decoded sequence:\n","[CLS] how many wins does the notre dame men's basketball team have? [SEP] championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were the most by the fighting irish team since 1908 - 09. [SEP]\n","The answer is not in this feature.\n","\n","\n"]}],"source":["answer = long_example[\"answers\"] # Retrieve the answer from the example\n","start_char = answer[\"answer_start\"][0] # Retrieve the index of the start character of the answer\n","end_char = start_char + len(answer[\"text\"][0]) # Retrieve the index of the end character of the answer\n","\n","# Iterate over the features\n","for i in range(len(tokenized_long_example[\"input_ids\"])):\n","  print(f\"Looking for the answer `{answer['text'][0]}` to the question `{long_example['question']}` in feature {i+1}.\")\n","  print(f\"The feature contains the following decoded sequence:\\n{tokenizer.decode(tokenized_long_example['input_ids'][i])}\")\n","  \n","  # Start token index of the current span in the text.\n","  token_start_index = 0\n","\n","  # --- START CODE HERE (05)\n","  # Find where the context sequence starts and store it in the variable token_start_index.\n","  while sequence_ids[token_start_index] != 1:\n","      token_start_index += 1\n","  # --- END CODE HERE\n","\n","  # --- START CODE HERE (06)\n","  # Find where the context sequence ends and store it in the variable token_end_index.\n","  token_end_index = len(tokenized_long_example[\"input_ids\"][i]) - 1\n","  while sequence_ids[token_end_index] != 1:\n","      token_end_index -= 1\n","  # --- END CODE HERE\n","\n","  offsets = tokenized_long_example[\"offset_mapping\"][i]\n","  # Detect if the answer is out of the span.\n","  if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","\n","      # --- START CODE HERE (07)\n","      # Find where are the start_position and end_position of the answer.\n","      # Move the token_start_index and token_end_index to the two ends of the answer.\n","      while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","          token_start_index += 1\n","      start_position = token_start_index - 1\n","      while offsets[token_end_index][1] >= end_char:\n","          token_end_index -= 1\n","      end_position = token_end_index + 1\n","      # --- END CODE HERE\n","\n","      print(f\"Answer found by the tokenizer at the token positions: {start_position}, {end_position}\")\n","      print(f\"{tokenizer.decode(tokenized_long_example['input_ids'][i][start_position: end_position+1])}\")\n","  else:\n","      print(\"The answer is not in this feature.\")\n","  print(\"\\n\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DSpLL3KUXsnr"},"source":["### Tokenize the whole dataset\n","\n","Now we can implement a function that will prepare the whole dataset following the above process."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"k2GyeNtAIt5j","executionInfo":{"status":"ok","timestamp":1666298429314,"user_tz":-120,"elapsed":21,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"}}},"outputs":[],"source":["def prepare_train_features(examples, tokenizer, max_length: int = 384, doc_stride: int = 128):\n","    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n","    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n","    # left whitespace\n","    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n","\n","    # For this notebook to work with any kind of models, we need to account for the special case where the model\n","    # expects padding on the left (in which case we switch the order of the question and the context)\n","    pad_on_right = tokenizer.padding_side == \"right\"\n","\n","    # --- START CODE HERE (08)\n","    # Apply the tokenizer as before except that be careful to correctly setup the order of question and context \n","    # given the value of the boolean pad_on_right.\n","    tokenized_examples = tokenizer(\n","        examples[\"question\" if pad_on_right else \"context\"],\n","        examples[\"context\" if pad_on_right else \"question\"],\n","        truncation=\"only_second\" if pad_on_right else \"only_first\",\n","        max_length=max_length,\n","        stride=doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","    # --- END CODE HERE\n","\n","    # Since one example might give us several features if it has a long context, we need a map from a feature to\n","    # its corresponding example. This key gives us just that.\n","    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n","    # The offset mappings will give us a map from token to character position in the original context. This will\n","    # help us compute the start_positions and end_positions.\n","    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n","\n","    # Let's label those examples!\n","    tokenized_examples[\"start_positions\"] = []\n","    tokenized_examples[\"end_positions\"] = []\n","\n","    # Iterate over the offset mapping from the features.\n","    for i, offsets in enumerate(offset_mapping):\n","        # We will label impossible answers with the index of the CLS token.\n","        input_ids = tokenized_examples[\"input_ids\"][i]\n","        cls_index = input_ids.index(tokenizer.cls_token_id)\n","\n","        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n","        sequence_ids = tokenized_examples.sequence_ids(i)\n","\n","        # One example can give several spans, this is the index of the example containing this span of text.\n","        sample_index = sample_mapping[i]\n","        answers = examples[\"answers\"][sample_index]\n","\n","        # --- START CODE HERE (09)\n","        # If no answers are given, set the cls_index as answer.\n","        if len(answers[\"answer_start\"]) == 0:\n","            tokenized_examples[\"start_positions\"].append(cls_index)\n","            tokenized_examples[\"end_positions\"].append(cls_index)\n","        # --- END CODE HERE\n","        else:\n","            # Start/end character index of the answer in the text.\n","            start_char = answers[\"answer_start\"][0]\n","            end_char = start_char + len(answers[\"text\"][0])\n","\n","            # --- START CODE HERE (10)\n","            # Find where the context sequence starts and ends as before. Be careful about the pad_on_right boolean.\n","            token_start_index = 0\n","            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n","                token_start_index += 1\n","\n","            token_end_index = len(input_ids) - 1\n","            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n","                token_end_index -= 1\n","            # --- END CODE HERE\n","\n","            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n","            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","                # --- START CODE HERE (11)\n","                # Label impossible answers with the index of the CLS token.\n","                tokenized_examples[\"start_positions\"].append(cls_index)\n","                tokenized_examples[\"end_positions\"].append(cls_index)\n","                # --- END CODE HERE\n","            else:\n","                # --- START CODE HERE (12)\n","                # Find where are the start_position and end_position of the answer as before\n","                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","                    token_start_index += 1\n","                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n","                while offsets[token_end_index][1] >= end_char:\n","                    token_end_index -= 1\n","                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n","                # --- END CODE HERE\n","            \n","\n","    return tokenized_examples"]},{"cell_type":"markdown","metadata":{"id":"N0jtaWMDE05A"},"source":["This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1666298429315,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"},"user_tz":-120},"id":"Qp3_0N3ML-Cq","outputId":"9407edd9-9aad-4a29-a4a5-b37dbd32497f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5, 384)"]},"metadata":{},"execution_count":18}],"source":["features = prepare_train_features(train_dataset[:5], tokenizer)\n","len(features[\"input_ids\"]), len(features[\"input_ids\"][0]) # should return (5, 384)"]},{"cell_type":"markdown","metadata":{"id":"C5wWMMHGMVYe"},"source":["Now, we can apply this function to our dataset using the [`.map`](https://huggingface.co/docs/datasets/v2.6.1/en/package_reference/main_classes#datasets.Dataset.map) operator from datasets to apply this tokenization process on our whole training dataset. \n","\n","We will apply the same function to the validation dataset to evaluate our model during training."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1666298429315,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"},"user_tz":-120},"id":"WdsIPpw_MSpv","outputId":"ab05b9d9-938a-4149-d7f9-8cf0a02a6d93"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-3be08ea5a4f5b0c4.arrow\n"]},{"output_type":"stream","name":"stdout","text":["9\n","<class 'datasets.arrow_dataset.Dataset'>\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-298865587200595e.arrow\n"]},{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [101,\n","  2000,\n","  3183,\n","  2106,\n","  1996,\n","  6261,\n","  2984,\n","  9382,\n","  3711,\n","  1999,\n","  8517,\n","  1999,\n","  10223,\n","  26371,\n","  2605,\n","  1029,\n","  102,\n","  6549,\n","  2135,\n","  1010,\n","  1996,\n","  2082,\n","  2038,\n","  1037,\n","  3234,\n","  2839,\n","  1012,\n","  10234,\n","  1996,\n","  2364,\n","  2311,\n","  1005,\n","  1055,\n","  2751,\n","  8514,\n","  2003,\n","  1037,\n","  3585,\n","  6231,\n","  1997,\n","  1996,\n","  6261,\n","  2984,\n","  1012,\n","  3202,\n","  1999,\n","  2392,\n","  1997,\n","  1996,\n","  2364,\n","  2311,\n","  1998,\n","  5307,\n","  2009,\n","  1010,\n","  2003,\n","  1037,\n","  6967,\n","  6231,\n","  1997,\n","  4828,\n","  2007,\n","  2608,\n","  2039,\n","  14995,\n","  6924,\n","  2007,\n","  1996,\n","  5722,\n","  1000,\n","  2310,\n","  3490,\n","  2618,\n","  4748,\n","  2033,\n","  18168,\n","  5267,\n","  1000,\n","  1012,\n","  2279,\n","  2000,\n","  1996,\n","  2364,\n","  2311,\n","  2003,\n","  1996,\n","  13546,\n","  1997,\n","  1996,\n","  6730,\n","  2540,\n","  1012,\n","  3202,\n","  2369,\n","  1996,\n","  13546,\n","  2003,\n","  1996,\n","  24665,\n","  23052,\n","  1010,\n","  1037,\n","  14042,\n","  2173,\n","  1997,\n","  7083,\n","  1998,\n","  9185,\n","  1012,\n","  2009,\n","  2003,\n","  1037,\n","  15059,\n","  1997,\n","  1996,\n","  24665,\n","  23052,\n","  2012,\n","  10223,\n","  26371,\n","  1010,\n","  2605,\n","  2073,\n","  1996,\n","  6261,\n","  2984,\n","  22353,\n","  2135,\n","  2596,\n","  2000,\n","  3002,\n","  16595,\n","  9648,\n","  4674,\n","  2061,\n","  12083,\n","  9711,\n","  2271,\n","  1999,\n","  8517,\n","  1012,\n","  2012,\n","  1996,\n","  2203,\n","  1997,\n","  1996,\n","  2364,\n","  3298,\n","  1006,\n","  1998,\n","  1999,\n","  1037,\n","  3622,\n","  2240,\n","  2008,\n","  8539,\n","  2083,\n","  1017,\n","  11342,\n","  1998,\n","  1996,\n","  2751,\n","  8514,\n","  1007,\n","  1010,\n","  2003,\n","  1037,\n","  3722,\n","  1010,\n","  2715,\n","  2962,\n","  6231,\n","  1997,\n","  2984,\n","  1012,\n","  102,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0],\n"," 'attention_mask': [1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0],\n"," 'start_positions': 130,\n"," 'end_positions': 137}"]},"metadata":{},"execution_count":19}],"source":["num_indices_to_keep_data = round(0.01 * len(train_dataset)) # 1% of data to keep.\n","indices = np.random.choice(range(len(train_dataset)), num_indices_to_keep_data, replace=False)\n","print(len(indices))\n","print(type(train_dataset))\n","subsample_train_dataset = train_dataset[indices]\n","\n","# --- START CODE HERE (13)\n","# Apply the prepare_train_features to the train_dataset and validation_dataset. Provide the tokenizer to the function and batch the data.\n","# Finally, remove the column names from the dataset.\n","tokenized_train_dataset = train_dataset.map(prepare_train_features, fn_kwargs={\"tokenizer\": tokenizer}, batched=True, remove_columns=train_dataset.column_names)\n","tokenized_validation_dataset = validation_dataset.map(prepare_train_features, fn_kwargs={\"tokenizer\": tokenizer}, batched=True, remove_columns=validation_dataset.column_names)\n","tokenized_train_dataset[0]\n","# --- END CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"YTiGQsdxNw5l"},"source":["## Fine-tune the model"]},{"cell_type":"markdown","source":["Now that we have transformed the dataset to feed the model, we will instantiate our model and train it.\n","\n","\n","First, we will retrieve the model thanks to the [auto model for question answering](https://huggingface.co/docs/transformers/model_doc/auto) from HuggingFace."],"metadata":{"id":"8RuRor-hAZPh"}},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1989,"status":"ok","timestamp":1666298431292,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"},"user_tz":-120},"id":"Jjt4VZkENzck","outputId":"219648a2-65ce-4412-a0b1-db231dcf2bdb"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n","- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# --- START CODE HERE (14)\n","# Import the correct auto model class and instantiate the model.\n","from transformers import AutoModelForQuestionAnswering\n","\n","model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n","# --- END CODE HERE"]},{"cell_type":"markdown","source":["To train a model, HuggingFace expects to instantiate a [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) taking as parameters:\n","- the model\n","- the arguments to configure the trainer\n","- the train dataset\n","- the eval dataset\n","- the default [data collator](https://huggingface.co/docs/transformers/main_classes/data_collator)\n","- the tokenizer\n","\n","First we will instantiate a [TrainerArguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) with the following parameters:\n","- evaluation at each epoch\n","- learning rate of value 2e-5\n","- batch size of 16 to train\n","- batch size of 16 to evaluate\n","- train for 5 epochs\n","- weight decay of 0.01"],"metadata":{"id":"b0v9PHLyBFyC"}},{"cell_type":"code","execution_count":21,"metadata":{"id":"8UUDDQbhOOHJ","executionInfo":{"status":"ok","timestamp":1666298431296,"user_tz":-120,"elapsed":14,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"}}},"outputs":[],"source":["from transformers import TrainingArguments\n","\n","model_name = model_checkpoint.split(\"/\")[-1]\n","\n","# --- START CODE HERE (14)\n","# Instantiate the Training Arguments.\n","args = TrainingArguments(\n","    f\"{model_name}-finetuned-squad\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=5,\n","    weight_decay=0.01,\n","    push_to_hub=False,\n",")\n","# --- END CODE HERE"]},{"cell_type":"markdown","source":["Now that we have the training arguments and the model, we can instantiate the trainer."],"metadata":{"id":"6eLFWUHGCSet"}},{"cell_type":"code","execution_count":22,"metadata":{"id":"-ZeTiBoJOU2K","executionInfo":{"status":"ok","timestamp":1666298433919,"user_tz":-120,"elapsed":2635,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"}}},"outputs":[],"source":["# --- START CODE HERE (15)\n","# Import the trainer and the data collator.\n","from transformers import Trainer, default_data_collator\n","# --- END CODE HERE\n","\n","# --- START CODE HERE (16)\n","# Instantiate the Trainer.\n","trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_train_dataset,\n","    eval_dataset=tokenized_validation_dataset,\n","    data_collator=default_data_collator,\n","    tokenizer=tokenizer,\n",")\n","# --- END CODE HERE"]},{"cell_type":"markdown","source":["Finally, we can launch the training that should last around 2 minutes to train."],"metadata":{"id":"WmCilHiDAB6F"}},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":790},"executionInfo":{"elapsed":167114,"status":"ok","timestamp":1666298601024,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"},"user_tz":-120},"id":"JXXtBCixPofm","outputId":"4de917b8-0a6e-4554-c3fb-3ef2c616e98e"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 908\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 285\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='285' max='285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [285/285 02:45, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>3.429340</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>3.054243</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>2.945534</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>2.785733</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>No log</td>\n","      <td>2.726093</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 106\n","  Batch size = 16\n","***** Running Evaluation *****\n","  Num examples = 106\n","  Batch size = 16\n","***** Running Evaluation *****\n","  Num examples = 106\n","  Batch size = 16\n","***** Running Evaluation *****\n","  Num examples = 106\n","  Batch size = 16\n","***** Running Evaluation *****\n","  Num examples = 106\n","  Batch size = 16\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=285, training_loss=3.307274748149671, metrics={'train_runtime': 166.8391, 'train_samples_per_second': 27.212, 'train_steps_per_second': 1.708, 'total_flos': 444873805608960.0, 'train_loss': 3.307274748149671, 'epoch': 5.0})"]},"metadata":{},"execution_count":23}],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"lpsRi1MwWecW"},"source":["### Evaluate our model"]},{"cell_type":"markdown","source":["After training our model, we can start evaluating it.\n","\n","For that we need to retrieve the prediction of our model. The following code gives us the keys returned by our model for a validation batch."],"metadata":{"id":"QpJp5HCobtrd"}},{"cell_type":"code","execution_count":24,"metadata":{"id":"2ZYiHiyOWhVy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666298601025,"user_tz":-120,"elapsed":44,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"}},"outputId":"91be18ba-a8a2-4bd1-f3dc-4d005ec9fb66"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['loss', 'start_logits', 'end_logits'])"]},"metadata":{},"execution_count":24}],"source":["import torch\n","\n","for batch in trainer.get_eval_dataloader():\n","    break\n","batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n","with torch.no_grad():\n","    output = trainer.model(**batch)\n","output.keys()"]},{"cell_type":"markdown","source":["Our model predicts two probability distributions over the tokens:\n","- the start token probability called the `start_logits`.\n","- the end token probability called the `end_logits`."],"metadata":{"id":"aYAwfmrWb8tC"}},{"cell_type":"code","source":["output.start_logits.shape, output.end_logits.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JvL-y0NUDJf5","executionInfo":{"status":"ok","timestamp":1666298601026,"user_tz":-120,"elapsed":34,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"}},"outputId":"ea062e91-7366-4158-8565-03c09999a369"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([16, 384]), torch.Size([16, 384]))"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["To output the actual hard prediction we take the argmax of each distribution. We can observe several issues:\n","- sometimes the end token predicted is before the start token which is impossible.\n","- the predicted token could be inside the question.\n","- if our context is too large, we will have several predictions for each feature provided by the tokenizer.\n","\n","Therefore, we need a procedure to select the best predictions."],"metadata":{"id":"_9-uR0wxcWgL"}},{"cell_type":"code","source":["output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ND8gfxVKDLTE","executionInfo":{"status":"ok","timestamp":1666298601028,"user_tz":-120,"elapsed":31,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"}},"outputId":"1b2aa430-2518-456a-ba90-51b9eba0f8b4"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([ 46,  46, 161, 161, 167, 162,  72,  42, 162,  41,  73, 159,  80, 163,\n","         170,  46], device='cuda:0'),\n"," tensor([ 47,  47,  44,  44,  50,  45,  44,  43,  45,  42,  13,  42,  46,  46,\n","         158,  47], device='cuda:0'))"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["In the following cell, we will make our logits follow this pipeline:\n","- keep the 20 best propositions for each `start_logits` and each `end_logits` (the maximum value of the probability distributions).\n","- make pair values of each `start_logits` and `end_logits` if the `end_logits` index is after the `start_logits` index.\n","\n","The idea is that every token proposition for both start and end should be taken into account and not only paired predictions especially when the first best paired predictions are not possible."],"metadata":{"id":"-AMGYKAodENU"}},{"cell_type":"code","source":["import numpy as np\n","\n","n_best_size = 20\n","start_logits = output.start_logits[0].cpu().numpy()\n","end_logits = output.end_logits[0].cpu().numpy()\n","\n","# --- START CODE HERE (17)\n","# Only keep the best propositions index.\n","start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n","end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n","# --- END CODE HERE\n","valid_answers = []\n","for start_index in start_indexes:\n","    for end_index in end_indexes:\n","        # --- START CODE HERE (18)\n","        # Only keep the valid pairs.\n","        if start_index <= end_index:\n","        # --- END CODE HERE\n","            valid_answers.append(\n","                {\n","                    \"score\": start_logits[start_index] + end_logits[end_index],\n","                    \"text\": \"\" # Later we will find a way to get back the original substring corresponding to the answer in the context\n","                }\n","            )\n","print(f\"We kept only {len(valid_answers)} valid pairs from {len(start_indexes) * len(end_indexes)} best pair propositions from {sum(start_logits.shape) * sum(end_logits.shape)} possible pairs.\")"],"metadata":{"id":"EOjrwZniDOMO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666298601028,"user_tz":-120,"elapsed":25,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"}},"outputId":"040c8b32-3180-4369-ded8-1d41a60313d2"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["We kept only 218 valid pairs from 400 best pair propositions from 147456 possible pairs.\n"]}]},{"cell_type":"markdown","source":["To retrieve all validation features, we need to add two things to our validation pipeline:\n","- verify that our pairs are inside the context and not the question.\n","- retrieve the actual text for the model instead of the tokens.\n","\n","We need to tokenize all our validation data. We will implement a process pipeline slightly different from `prepare_train_features` that we implemented before."],"metadata":{"id":"zmFzg4-Ufh22"}},{"cell_type":"code","source":["def prepare_validation_features(examples):\n","    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n","    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n","    # left whitespace\n","    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n","\n","    # For this notebook to work with any kind of models, we need to account for the special case where the model\n","    # expects padding on the left (in which case we switch the order of the question and the context)\n","    pad_on_right = tokenizer.padding_side == \"right\"\n","\n","    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n","    # in one example possible giving several features when a context is long, each of those features having a\n","    # context that overlaps a bit the context of the previous feature.\n","    tokenized_examples = tokenizer(\n","        examples[\"question\" if pad_on_right else \"context\"],\n","        examples[\"context\" if pad_on_right else \"question\"],\n","        truncation=\"only_second\" if pad_on_right else \"only_first\",\n","        max_length=max_length,\n","        stride=doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    # --- START CODE HERE (19)\n","    # Apply the tokenizer as for prepare_train_features\n","    tokenized_examples = tokenizer(\n","        examples[\"question\" if pad_on_right else \"context\"],\n","        examples[\"context\" if pad_on_right else \"question\"],\n","        truncation=\"only_second\" if pad_on_right else \"only_first\",\n","        max_length=max_length,\n","        stride=doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","    # --- END CODE HERE\n","\n","    # Since one example might give us several features if it has a long context, we need a map from a feature to\n","    # its corresponding example. This key gives us just that.\n","    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n","\n","    # We keep the example_id that gave us this feature and we will store the offset mappings.\n","    tokenized_examples[\"example_id\"] = []\n","\n","    for i in range(len(tokenized_examples[\"input_ids\"])):\n","        # --- START CODE HERE (20)\n","        # Grab the text sequence corresponding to that feature.\n","        sequence_ids = tokenized_examples.sequence_ids(i)\n","        # --- END CODE HERE\n","\n","        context_index = 1 if pad_on_right else 0\n","\n","        # One example can give several spans, this is the index of the example containing this span of text.\n","        sample_index = sample_mapping[i]\n","        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n","\n","        # --- START CODE HERE (21)\n","        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n","        # position is part of the context or not.\n","        tokenized_examples[\"offset_mapping\"][i] = [\n","            (o if sequence_ids[k] == context_index else None)\n","            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n","        ]\n","        # --- END CODE HERE\n","\n","\n","    return tokenized_examples"],"metadata":{"id":"kL39lTERDQFm","executionInfo":{"status":"ok","timestamp":1666298601029,"user_tz":-120,"elapsed":22,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["Now, we can apply this function to our dataset using the [`.map`](https://huggingface.co/docs/datasets/v2.6.1/en/package_reference/main_classes#datasets.Dataset.map) operator from datasets to apply this tokenization process on our whole validation dataset. "],"metadata":{"id":"HKUbt2C2-KUv"}},{"cell_type":"code","source":["# --- START CODE HERE (22)\n","# Apply the prepare_validation_features to the validation_dataset. Provide the tokenizer to the function and batch the data.\n","# Finally, remove the column names from the dataset.\n","validation_features = validation_dataset.map(\n","    prepare_validation_features,\n","    batched=True,\n","    remove_columns=validation_dataset.column_names\n",")\n","# --- END CODE HERE"],"metadata":{"id":"-KkBlNL-DRbB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666298601029,"user_tz":-120,"elapsed":21,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"}},"outputId":"601412b0-df29-491f-bee8-98a7873b7a9b"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-c73457f0f4c18052.arrow\n"]}]},{"cell_type":"markdown","source":["With the `validation_features`, we will make predictions thanks to the [trainer](https://huggingface.co/docs/transformers/main_classes/trainer)."],"metadata":{"id":"SkdVBZv8-RYz"}},{"cell_type":"code","source":["raw_predictions = trainer.predict(validation_features)"],"metadata":{"id":"10dT73aXDRqo","colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"status":"ok","timestamp":1666298603495,"user_tz":-120,"elapsed":2482,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"}},"outputId":"824f4c4c-4288-40b2-c4d9-145372109fa7"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stderr","text":["The following columns in the test set don't have a corresponding argument in `DistilBertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `DistilBertForQuestionAnswering.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 106\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}}]},{"cell_type":"markdown","source":["The `Trainer` *hides* the columns that are not used by the model (here `example_id` and `offset_mapping` which we will need for our post-processing), so we set them back:"],"metadata":{"id":"2MKPUMtz-smj"}},{"cell_type":"code","source":["validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"],"metadata":{"id":"8OXr2bXgDS4q","executionInfo":{"status":"ok","timestamp":1666298603497,"user_tz":-120,"elapsed":34,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["To refine our validation pipeline and eliminate irrelevant answers we will filter:\n","- the answers containing `None` in the offset mappings as it corresponds to a part of the question\n","- the answers longer than the hyper-parameter `max_answer_length`"],"metadata":{"id":"tvB5Ny6Q-7Kk"}},{"cell_type":"code","source":["max_answer_length = 30\n","start_logits = output.start_logits[0].cpu().numpy()\n","end_logits = output.end_logits[0].cpu().numpy()\n","offset_mapping = validation_features[0][\"offset_mapping\"]\n","\n","# The first feature comes from the first example. For the more general case, we will need to be match the example_id to\n","# an example index\n","context = validation_dataset[0][\"context\"]\n","\n","# --- START CODE HERE (23)\n","# Only keep the best propositions index as before.\n","start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n","end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n","# --- END CODE HERE\n","valid_answers = []\n","for start_index in start_indexes:\n","    for end_index in end_indexes:\n","        # --- START CODE HERE (24)\n","        # Filter out-of-scope answers: indices out of bounds or in the question.\n","        if (\n","            start_index >= len(offset_mapping)\n","            or end_index >= len(offset_mapping)\n","            or offset_mapping[start_index] is None\n","            or offset_mapping[end_index] is None\n","        ):\n","        # --- END CODE HERE\n","            continue\n","\n","        # --- START CODE HERE (25)\n","        # Consider answers that are valid and shorter than max_answer_length.\n","        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n","        # --- END CODE HERE\n","            continue\n","\n","        start_char = offset_mapping[start_index][0]\n","        end_char = offset_mapping[end_index][1]\n","        valid_answers.append(\n","            {\n","                \"score\": start_logits[start_index] + end_logits[end_index],\n","                \"text\": context[start_char: end_char]\n","            }\n","        )\n","\n","valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n","valid_answers"],"metadata":{"id":"owTrlO2ODWwm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666298603498,"user_tz":-120,"elapsed":34,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"}},"outputId":"0e0dbdfc-8cce-490e-e16f-295ba01afe0f"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 5.36228, 'text': 'Denver Broncos'},\n"," {'score': 4.343049,\n","  'text': 'American Football Conference (AFC) champion Denver Broncos'},\n"," {'score': 3.748499,\n","  'text': 'Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016'},\n"," {'score': 3.233025, 'text': 'Super Bowl L'},\n"," {'score': 3.0383232,\n","  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n"," {'score': 3.0111775, 'text': 'Broncos'},\n"," {'score': 3.0016599,\n","  'text': '2015 season. The American Football Conference (AFC) champion Denver Broncos'},\n"," {'score': 3.0003338,\n","  'text': 'National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos'},\n"," {'score': 2.9524784, 'text': '2016'},\n"," {'score': 2.93812, 'text': 'February 7, 2016'},\n"," {'score': 2.9262085,\n","  'text': 'National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016'},\n"," {'score': 2.8564906, 'text': 'Super Bowl 50'},\n"," {'score': 2.8238878,\n","  'text': 'Roman numerals (under which the game would have been known as \"Super Bowl L'},\n"," {'score': 2.7928715, 'text': 'L'},\n"," {'score': 2.7471123, 'text': 'Arabic numerals 50'},\n"," {'score': 2.41854, 'text': 'Carolina Panthers'},\n"," {'score': 2.398705,\n","  'text': \"Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium\"},\n"," {'score': 2.389615,\n","  'text': 'Denver Broncos defeated the National Football Conference'},\n"," {'score': 2.3521066, 'text': \"Levi's Stadium\"},\n"," {'score': 2.0741203, 'text': '50'}]"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["We can compare to the actual ground-truth answer:"],"metadata":{"id":"42S5EeXRAsMN"}},{"cell_type":"code","source":["validation_dataset[0][\"answers\"]"],"metadata":{"id":"BLARf1g6DYSw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666298603501,"user_tz":-120,"elapsed":30,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"}},"outputId":"139c5b57-2b1e-4bb5-bfc1-692f9388e5fe"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n"," 'answer_start': [177, 177, 177]}"]},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","source":["As mentioned in the code above, this was easy on the first feature because we knew it comes from the first example.\n","\n","For the other features, we will map between examples and their corresponding features. Since one example can give several features, we will gather together all the answers in all the features generated by a given example, then pick the best one. The following code builds a map from example index to its corresponding features indices:"],"metadata":{"id":"awMpb9o6A242"}},{"cell_type":"code","source":["import collections\n","\n","features = validation_features\n","\n","example_id_to_index = {k: i for i, k in enumerate(validation_dataset[\"id\"])}\n","features_per_example = collections.defaultdict(list)\n","for i, feature in enumerate(features):\n","    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"],"metadata":{"id":"9l9GtlcZDZVd","executionInfo":{"status":"ok","timestamp":1666298603502,"user_tz":-120,"elapsed":27,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["All combined together, this gives us this post-processing function:"],"metadata":{"id":"ADFQzsTwBDCs"}},{"cell_type":"code","source":["from tqdm.auto import tqdm\n","\n","def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n","    all_start_logits, all_end_logits = raw_predictions\n","    # Build a map example to its corresponding features.\n","    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n","    features_per_example = collections.defaultdict(list)\n","    for i, feature in enumerate(features):\n","        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n","\n","    # The dictionaries we have to fill.\n","    predictions = collections.OrderedDict()\n","\n","    # Logging.\n","    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n","\n","    # Let's loop over all the examples!\n","    for example_index, example in enumerate(tqdm(examples)):\n","        # Those are the indices of the features associated to the current example.\n","        feature_indices = features_per_example[example_index]\n","\n","        min_null_score = None # Only used if squad_v2 is True.\n","        valid_answers = []\n","        \n","        context = example[\"context\"]\n","        # Looping through all the features associated to the current example.\n","        for feature_index in feature_indices:\n","            # We grab the predictions of the model for this feature.\n","            start_logits = all_start_logits[feature_index]\n","            end_logits = all_end_logits[feature_index]\n","            # This is what will allow us to map some the positions in our logits to span of texts in the original\n","            # context.\n","            offset_mapping = features[feature_index][\"offset_mapping\"]\n","\n","            # Update minimum null prediction.\n","            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n","            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n","            if min_null_score is None or min_null_score < feature_null_score:\n","                min_null_score = feature_null_score\n","\n","            # --- START CODE HERE (26)\n","            # Only keep the best propositions index as before.\n","            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n","            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n","            # --- END CODE HERE\n","            for start_index in start_indexes:\n","                for end_index in end_indexes:\n","                    # --- START CODE HERE (27)\n","                    # Filter out-of-scope answers: indices out of bounds or in the question as before.\n","                    if (\n","                        start_index >= len(offset_mapping)\n","                        or end_index >= len(offset_mapping)\n","                        or offset_mapping[start_index] is None\n","                        or offset_mapping[end_index] is None\n","                    ):\n","                    # --- END CODE HERE\n","                        continue\n","                    # --- START CODE HERE (28)\n","                    # Consider valid answers and the ones that have length shorter than max_answer_length.\n","                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n","                    # --- END CODE HERE\n","                        continue\n","\n","                    start_char = offset_mapping[start_index][0]\n","                    end_char = offset_mapping[end_index][1]\n","                    valid_answers.append(\n","                        {\n","                            \"score\": start_logits[start_index] + end_logits[end_index],\n","                            \"text\": context[start_char: end_char]\n","                        }\n","                    )\n","        \n","        if len(valid_answers) > 0:\n","            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n","        else:\n","            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n","            # failure.\n","            best_answer = {\"text\": \"\", \"score\": 0.0}\n","        \n","        # Let's pick our final answer: the best one\n","        predictions[example[\"id\"]] = best_answer[\"text\"]\n","\n","    return predictions"],"metadata":{"id":"9M-aoJgmDa9t","executionInfo":{"status":"ok","timestamp":1666298603503,"user_tz":-120,"elapsed":27,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["Now, we can apply the post-processing function to our predictions:"],"metadata":{"id":"67yWNaXhCPTN"}},{"cell_type":"code","source":["# --- START CODE HERE (29)\n","# Apply our postprocess_qa_predictions to our predictions.\n","final_predictions = postprocess_qa_predictions(validation_dataset, validation_features, raw_predictions.predictions)\n","# --- END CODE HERE"],"metadata":{"id":"uCZtOtc6DcaF","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["3549d7a4ba734be2b110741b9744179d","497e09b2c6cf41b0ab7d171f1befa78b","213def8fb5c643d2ac2303656a4d82f2","efbe6fe898234fe58f76108e656e0747","4b67adcdc91c48ac8f671e07ea834d6d","501b3dccd9dd424188056b122c68a123","007d31e292cb44c48e5d661c1193755b","12aadcd8c23b4b158dd2f3c42c6e4538","6c6a4668610248b290d6d74e8dd782ba","21cdb9557f404e0c90d4c1278bfa3a2f","89b063b17d1444528cefa87d50fbb2bb"]},"executionInfo":{"status":"ok","timestamp":1666298603504,"user_tz":-120,"elapsed":28,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"}},"outputId":"af92e42d-c2d1-4bb6-b4d0-c95d4ffcf32d"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Post-processing 106 example predictions split into 106 features.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/106 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3549d7a4ba734be2b110741b9744179d"}},"metadata":{}}]},{"cell_type":"markdown","source":["Then we can load the metric from the datasets library."],"metadata":{"id":"EW278jRADHJg"}},{"cell_type":"code","source":["from datasets import load_metric\n","\n","metric = load_metric(\"squad\")"],"metadata":{"id":"ak48Ci_CDdk0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666298604603,"user_tz":-120,"elapsed":1123,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"}},"outputId":"bd3063df-9c37-4b27-b41c-85488f2f148d"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"]}]},{"cell_type":"code","source":["formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n","references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in validation_dataset]\n","metric.compute(predictions=formatted_predictions, references=references)"],"metadata":{"id":"0QNRI8RQDepx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666298604604,"user_tz":-120,"elapsed":10,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"}},"outputId":"19c5a79c-3178-460c-fcaa-be4bf72f26b9"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'exact_match': 39.62264150943396, 'f1': 42.6284067085954}"]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","source":["You can improve this result by having a larger dataset or train longer !"],"metadata":{"id":"S9nqJbZ2CIwG"}},{"cell_type":"markdown","metadata":{"id":"MLJvhtkY1gQT"},"source":["## What to do now ?\n","\n","If you want, you can lookup for datasets in your own language and see if distilbert performs correctly. Generally, a model that was learnt on the same language as your dataset will work better than a general model that was learnt on several languages or, obviously, on a totally different language. \n","\n","For example for french, Camembert is a BERT model but trained on french datasets and obtain a very good performance for french NLP tasks.\n","\n","You can take a look at other HuggingFace tutorial that cover other tasks to see what is the tokenization process, how the model is different for such tasks:\n","- [translation](https://huggingface.co/docs/transformers/tasks/translation)\n","- [summarization](https://huggingface.co/docs/transformers/tasks/summarization)\n","- ...\n","\n"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"yQN_3Hp53KLY","executionInfo":{"status":"ok","timestamp":1666298604604,"user_tz":-120,"elapsed":8,"user":{"displayName":"Julien Denize","userId":"01289477169777414372"}}},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOhU08CgzderUi8rNVL3WFt"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"046230d261a64745accbf38b400f2a67":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fe9c6020a6a74ad4aab243f644f04b49","IPY_MODEL_0f54d1a500ba4bb5abac1a817e8df539","IPY_MODEL_fd91c69345a64a2d9b82153204c7d8a3"],"layout":"IPY_MODEL_464b34dacbf6413099b2dc3182bec6a9"}},"fe9c6020a6a74ad4aab243f644f04b49":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c705522c27e4febb57b404cba504a58","placeholder":"​","style":"IPY_MODEL_b71ffc739b37438cb5c3cbd00469126d","value":"100%"}},"0f54d1a500ba4bb5abac1a817e8df539":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_53e240692d0744e595d679342e1a8125","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ee6ecee411b24f03a2777a8c1b16b59f","value":2}},"fd91c69345a64a2d9b82153204c7d8a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6104cf999864ba2b42a531ac3a53d97","placeholder":"​","style":"IPY_MODEL_2b1cae93df604dd0a8a0871123543e0b","value":" 2/2 [00:00&lt;00:00, 41.09it/s]"}},"464b34dacbf6413099b2dc3182bec6a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c705522c27e4febb57b404cba504a58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b71ffc739b37438cb5c3cbd00469126d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53e240692d0744e595d679342e1a8125":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee6ecee411b24f03a2777a8c1b16b59f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a6104cf999864ba2b42a531ac3a53d97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b1cae93df604dd0a8a0871123543e0b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a06763d3c344458084fa5f5b5f3e8c4b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e03463bf84f946c1b727a3bcbd814d77","IPY_MODEL_0cbac09bea6243988a52444250349820","IPY_MODEL_e5ee9fbe20e04648b18fc4028da8f7be"],"layout":"IPY_MODEL_19dd94bd50a241baaa96e9db2a473641"}},"e03463bf84f946c1b727a3bcbd814d77":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b18fb29df864c768aa813c186540d8c","placeholder":"​","style":"IPY_MODEL_c17098156ca64c2b947843d12b172a8c","value":""}},"0cbac09bea6243988a52444250349820":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d54c41fbf13a477baa8897a6b514a097","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_af3699612ed54d84950757ec28db226c","value":0}},"e5ee9fbe20e04648b18fc4028da8f7be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6e68e9608454b4fb395f14fc475ba1e","placeholder":"​","style":"IPY_MODEL_6cd6e76609d2484ba2bc329efb7342f5","value":" 0/0 [00:00&lt;?, ?it/s]"}},"19dd94bd50a241baaa96e9db2a473641":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b18fb29df864c768aa813c186540d8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c17098156ca64c2b947843d12b172a8c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d54c41fbf13a477baa8897a6b514a097":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"af3699612ed54d84950757ec28db226c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d6e68e9608454b4fb395f14fc475ba1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cd6e76609d2484ba2bc329efb7342f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3549d7a4ba734be2b110741b9744179d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_497e09b2c6cf41b0ab7d171f1befa78b","IPY_MODEL_213def8fb5c643d2ac2303656a4d82f2","IPY_MODEL_efbe6fe898234fe58f76108e656e0747"],"layout":"IPY_MODEL_4b67adcdc91c48ac8f671e07ea834d6d"}},"497e09b2c6cf41b0ab7d171f1befa78b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_501b3dccd9dd424188056b122c68a123","placeholder":"​","style":"IPY_MODEL_007d31e292cb44c48e5d661c1193755b","value":"100%"}},"213def8fb5c643d2ac2303656a4d82f2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_12aadcd8c23b4b158dd2f3c42c6e4538","max":106,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6c6a4668610248b290d6d74e8dd782ba","value":106}},"efbe6fe898234fe58f76108e656e0747":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_21cdb9557f404e0c90d4c1278bfa3a2f","placeholder":"​","style":"IPY_MODEL_89b063b17d1444528cefa87d50fbb2bb","value":" 106/106 [00:00&lt;00:00, 424.36it/s]"}},"4b67adcdc91c48ac8f671e07ea834d6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"501b3dccd9dd424188056b122c68a123":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"007d31e292cb44c48e5d661c1193755b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"12aadcd8c23b4b158dd2f3c42c6e4538":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c6a4668610248b290d6d74e8dd782ba":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"21cdb9557f404e0c90d4c1278bfa3a2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89b063b17d1444528cefa87d50fbb2bb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}