{"cells":[{"cell_type":"markdown","metadata":{"id":"KzZZIjnrUD2C"},"source":["# TP3: Generative Models in Pytorch\n","\n","**Authors:** \n","- julien.denize@centralesupelec.fr\n","- tom.dupuis@centralesupelec.fr\n","\n","\n","If you have questions or suggestions, contact us and we will gladly answer and take into account your remarks.\n","\n","For this tp you need to have some ground understanding of pytorch and basics introduction. It is available [here](https://pytorch.org/tutorials/beginner/basics/intro.html).\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4PCUqniI_XdU"},"source":["## Objective\n","\n","In this TP, we will implement Generative Adversarial Networks (GANs) to generate new images based on the [MNIST dataset](http://yann.lecun.com/exdb/mnist/).\n","\n","As a bonus, we will also implement a Variational Auto Encoder (VAE) to compare results from both methods.\n","\n","The MNIST database of handwritten digits has a training set of 60 000 examples, and a test set of 10 000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. \n"]},{"cell_type":"markdown","metadata":{"id":"3pTSu16X_Ujp"},"source":["## Your task\n","\n","Fill the missing parts in the code (parts between # --- START CODE HERE and # --- END CODE HERE)"]},{"cell_type":"markdown","metadata":{"id":"cbM55vbcbEFY"},"source":["## Retrieve MNIST dataset"]},{"cell_type":"markdown","metadata":{"id":"mt5wCw3cxjyi"},"source":["### Load the dataset\n","\n","Before diving into GANs and training our model, we need to load the data.\n","\n","As said before we will use the MNIST dataset for training (and not testing as we don't need it). We will first load the data using the torchvision [MNIST dataset](https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html).\n","\n","We will apply the transform to convert to tensor the images."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"LfMq8LqwT8eZ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100.0%\n"]},{"name":"stdout","output_type":"stream","text":["Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100.0%\n","2.0%"]},{"name":"stdout","output_type":"stream","text":["Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100.0%\n"]},{"name":"stdout","output_type":"stream","text":["Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100.0%"]},{"name":"stdout","output_type":"stream","text":["Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# --- START CODE HERE (01)\n","# Import dataset and transforms\n","from torchvision.datasets import MNIST\n","from torchvision.transforms import ToTensor\n","# --- END CODE HERE\n","\n","# --- START CODE HERE (02)\n","# Instantiate the dataset.\n","transform = ToTensor()\n","\n","dataset_train = MNIST(root='data', train=True, download=True, transform=transform)\n","# --- END CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"ljVwl0PIeOb5"},"source":["### Visualization\n","\n","We provide below the functions to visualize data and metrics. We also provide a function to initialize our networks later.\n","\n","You can toy with the functions to visualize the different parts of the dataset and later to display generated images."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"04hnwRWDeXYD"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","import numpy as np\n","import torch\n","import torchvision\n","from matplotlib import rc\n","from torchvision.utils import make_grid\n","from torch import Tensor\n","from typing import List\n","\n","rc('animation', html='jshtml')\n","\n","# We define here an util function to initialize the weights of our network.\n","# This function is not related to visualization.\n","def weights_init(m):\n","  classname = m.__class__.__name__\n","  if classname.find('Conv') != -1:\n","    m.weight.data.normal_(0.0, 0.02)\n","  elif classname.find('BatchNorm') != -1:\n","    m.weight.data.normal_(1.0, 0.02)\n","    m.bias.data.fill_(0)\n","\n","\n","def display_losses_and_accuracies_gan(discriminator_loss, generator_loss, real_accuracy, fake_accuracy):\n","  \"\"\"\n","  Display the losses and accuracies from our GAN model.\n","\n","  Args:\n","    discriminator_loss: the discriminator loss list.\n","    generator_loss: the generator loss list.\n","    real_accuracy: the real accuracy from our discriminator list.\n","    fake_accuracy: the fake accuracy from our discriminator list.\n","  \"\"\"\n","  plt.figure(figsize=(10,4))\n","  plt.subplot(1,2,1)\n","  plt.plot(discriminator_loss, 'r', label='discriminator')\n","  plt.plot(generator_loss, 'g--', label='generator')\n","  plt.xlabel('# epoch')\n","  plt.ylabel('loss')\n","  plt.grid(True)\n","  plt.legend()\n","\n","  plt.subplot(1,2,2)\n","  plt.plot(real_accuracy, 'r', label='real')\n","  plt.plot(fake_accuracy, 'g--', label='fake')\n","  plt.xlabel('# epoch')\n","  plt.ylabel('accuracy')\n","  plt.grid(True)\n","  plt.legend()\n","  plt.show()\n","\n","\n","def display_losses_and_accuracies_vae(kl_loss, recon_loss):\n","  \"\"\"\n","  Display the losses from our VAE model.\n","\n","  Args:\n","    kl_loss: the kl loss list.\n","    recon_loss: the reconstruction loss list.\n","  \"\"\"\n","  plt.figure(figsize=(10,4))\n","  plt.subplot(1,2,1)\n","  plt.plot(kl_loss, 'r', label='kl loss')\n","  plt.xlabel('# epoch')\n","  plt.ylabel('loss')\n","  plt.grid(True)\n","  plt.legend()\n","\n","  plt.subplot(1,2,2)\n","  plt.plot(recon_loss, 'g--', label='reconstruction loss')\n","  plt.xlabel('# epoch')\n","  plt.ylabel('loss')\n","  plt.grid(True)\n","  plt.legend()\n","  plt.show()\n","\n","\n","def compute_accuracy(preds: Tensor, y: Tensor) -> Tensor:\n","  \"\"\"\n","  Compute the accuracy.\n","  \n","  Args\n","    preds: predicted value by the discriminator.\n","    y: ground-truth class to predict.\n","  Returns:\n","    The accuracy.\n","  \"\"\"\n","  \n","  hard_preds = (preds > 0.5).long().float()\n","  return torch.mean((hard_preds == y).long().float())\n","\n","\n","def denormalize_tensor(tensor, mean, std) -> Tensor:\n","  \"\"\"\n","  Denormalize a tensor.\n","\n","  Args:\n","    tensor: the tensor to denormalize.\n","    mean: the mean that normalized.\n","    std: the std that normalized.\n","  Return:\n","    The denormalized tensor.\n","  \"\"\"\n","  \n","  mean = torch.tensor(mean).view((1, 1, 1, 1))\n","  std = torch.tensor(std).view((1, 1, 1 , 1))\n","  return tensor * std + mean\n","\n","\n","def show(imgs) -> None:\n","  \"\"\"\n","  Show input images.\n","\n","  Args:\n","    imgs: the images to show\n","  \"\"\"\n","  \n","  if not isinstance(imgs, list):\n","      imgs = [imgs]\n","  fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=(8,8))\n","  plt.axis('off')\n","  for i, img in enumerate(imgs):\n","      img = img.detach()\n","      img = torchvision.transforms.functional.to_pil_image(img)\n","      axs[0, i].imshow(255 - np.asarray(img), cmap='gray_r')\n","      axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n","\n","\n","def display_tensors_in_grid(tensors, nrows=5, padding=10) -> None:\n","  \"\"\"\n","  Convert tensors images into grid and display the grid.\n","\n","  Args:\n","    tensors: The tensors images to display.\n","    nrows: The number of rows in the grid.\n","    padding: The padding pixels added around each image.\n","  \"\"\"\n","\n","  grid = make_grid(tensors, nrow=nrows, padding=padding)\n","  show(grid)\n","\n","\n","def display_fake_images_in_grid(list_tensors, nrows=5, padding=10):\n","  \"\"\"\n","  Convert tensors images into grid and display the grid.\n","\n","  Args:\n","    list_tensors: The tensors images to display.\n","    nrows: The number of rows in the grid.\n","    padding: The padding pixels added around each image.\n","  \"\"\"\n","\n","  for tensor in list_tensors:\n","    grid = make_grid((tensor * 255), nrow=nrows, padding=padding)\n","    show(grid)\n","\n","\n","def get_slide_show_fake_images_in_grid(fake_images: List[Tensor], every_n_grid: int = 10) -> animation.ArtistAnimation:\n","  \"\"\"\n","  Get the animation to display given fake images as grids.\n","\n","  Args:\n","    fake_images: the list of images to display.\n","    every_n_grid: show grids modulo this parameter to prevent animation RAM error.\n","\n","  Return:\n","    the slide show of fake images in grid.\n","  \"\"\"\n","\n","  kept_fake_images = [fake_image for i,fake_image in enumerate(fake_images) if i % (every_n_grid - 1) == 0]\n","  cat_fake_images = torch.stack([make_grid(fake_image, nrow=5, padding=10).permute(1,2,0) for fake_image in kept_fake_images])\n","  cat_fake_images = 255 - (cat_fake_images * 255).cpu().long()\n","  fig, ax = plt.subplots(figsize=(8,8))\n","  frames = [[ax.imshow(cat_fake_images[i], cmap=\"gray\")] for i in range(len(cat_fake_images))]\n","  ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n","  anim = animation.ArtistAnimation(fig,frames)\n","  plt.close()\n","  return anim"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"pEqCXjare5zj"},"outputs":[{"name":"stdout","output_type":"stream","text":["Grid of train images:\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 800x800 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["random_train_samples = [dataset_train[idx] for idx in np.random.choice(len(dataset_train), 25)]\n","random_train_X = torch.stack([sample[0] for sample in random_train_samples])\n","train_grid = display_tensors_in_grid(random_train_X)\n","\n","print(f\"Grid of train images:\")"]},{"cell_type":"markdown","metadata":{"id":"lnPVZhYiDA1y"},"source":["## Generative Adversarial Network (GAN)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IoAPIJbrTmi4"},"source":["### Introduction\n","\n","GANs have been first proposed in 2014 by [Ian Goodfellow et al](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf). \n","\n","They consist of two neural networks that learn adversarial tasks.\n","- The **Generator** (G) learns to generate data.\n","- The **Discriminator** (D) learns to distinguish between fake data generated by the generator and real data.\n","\n","Our objective will be to generate new figure images for MNIST thanks to the Generator network of a GAN.\n","\n","![](https://files.realpython.com/media/fig_generative.5f01c08f5208.png)\n","\n","To generate new data, generative models generally takes as an input a random vector drawn from a specific distribution and the generator outputs accordingly to this distribution an image.\n","\n","To train a GAN, the generator and the discriminator are trained at different steps. There could be several steps for updating the generator before updating the discriminator.  To simply training we will perform the training of the whole GAN by doing for each iteration:\n","- 1 step of Generator update\n","- 1 step of Discriminator update"]},{"cell_type":"markdown","metadata":{"id":"UcVlB74JTq22"},"source":["### Train the discriminator\n","\n","To train the discriminator you must feed it:\n","- Several real samples labeled 1\n","- Several fake samples from G labeled 0\n","\n","The discriminator will then try to predict which samples are real or fake.\n","\n","Typically the discriminator outputs a scalar prediction $o_i \\in \\mathbb{R}$ on which the sigmoid is applied to make the prediction $\\hat{y_i} = \\frac{1}{1+e^{o_i}}$ for each element in a batch.\n","\n","The loss function optimized by the discriminator is the cross-entropy loss:\n","$$L_{D} = - \\frac{1}{N} \\sum_{i=1}^N \\left(y_i \\log(\\hat{y_i}) + (1 - y_i) log(1 - \\hat{y}_i)\\right)$$\n","with $N$ the batch size and $y = \\{y_i\\}_{i\\in[1,N]}$ the label 1 for real image and 0 for fake ones.\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0iO6w8GcTxr3"},"source":["### Train the Generator\n","\n","To train G, the steps are:\n","- Take as input random vectors\n","- Generate new samples\n","- Give generate samples to D\n","\n","The objective function of G is to **fool** D. During the update of G, D is frozen. Therefore the generator wants to **maximize** the cross-entropy loss when the input is fake:\n","$$L_{G} = \\mathbf{+} \\frac{1}{N} \\sum_{i=1}^N log(1 - \\hat{y}_i)$$\n","\n","However if the generator is too good, the above loss is near 0 and produce small gradients. In practice we **minimize** the following loss when the input is fake:\n","\n","$$L_{G} = - \\frac{1}{N} \\sum_{i=1}^N log(\\hat{y}_i)$$\n","\n","This loss consists in feeding the discriminator a fake data but with label 1 (real label).\n","\n","If your GAN is training well, you should see throughout training that G is generating better images and that D is having trouble to distinguish generated images from real images.\n"]},{"cell_type":"markdown","metadata":{"id":"bjyFWPIxTvdh"},"source":["### Implementation\n","\n","For this, we will not use [Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) models from Pytorch but subclass a pytorch [Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) to create a Generator, a Discriminator.\n","\n","Modules allow us to encapsulate the model definition, the forward pass (which is passing the input through the network to compute gradients) and more if you want to (the loss computation for example).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mnpaGDZgULJh"},"source":["#### Implementation of the Generator\n","\n","The Generator is a neural network that takes as an input a random noise of size 100. We will increase its dimensionality, reshape it according to a grid and use several convolutional transposed layers to upsample its resolution to reach the size of the MNIST dataset and generate an image thanks to the sigmoid activation function.\n","\n","The architecture will be the following:\n","- a Fully-Connected layer of output size $128 \\times 7 \\times 7$\n","- a LeakyReLU layer with $\\alpha = 0.2$\n","- a Reshape is applied to arrange data according to a grid of shape (128, 7, 7)\n","- a Convolutional Transposed layer of 128 filters of shape (4,4) and stride (2,2) and padding (1,1)\n","- a LeakyReLU layer with $\\alpha = 0.2$\n","- a Convolutional Transposed layer of 128 filters of shape (4,4) and stride (2,2) and padding (1,1)\n","- a LeakyReLU layer with $\\alpha = 0.2$\n","- a Convolutional layer of 1 filter of shape (7,7) and same padding\n","- a Sigmoid activation layer"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"XlhILfNHGknf"},"outputs":[{"data":{"text/plain":["Generator(\n","  (fc): Linear(in_features=100, out_features=6272, bias=True)\n","  (leaky_relu_1): LeakyReLU(negative_slope=0.2)\n","  (convt_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  (leaky_relu_2): LeakyReLU(negative_slope=0.2)\n","  (convt_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  (leaky_relu_3): LeakyReLU(negative_slope=0.2)\n","  (conv): Conv2d(128, 1, kernel_size=(7, 7), stride=(1, 1), padding=same)\n","  (sig): Sigmoid()\n",")"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from torch import Tensor\n","# --- START CODE HERE (03)\n","# Import the needed layers and the Module base class\n","from torch.nn import Conv2d, ConvTranspose2d, LeakyReLU, Linear, Module, Sigmoid\n","# --- END CODE HERE\n","\n","class Generator(Module):\n","  def __init__(self, noise_dim: int = 100) -> None:\n","    \"\"\"\n","    Create a generator.\n","\n","    Args:\n","      noise_dim: the dimension of the input noise.\n","    \"\"\"\n","    super().__init__()\n","\n","    # --- START CODE HERE (04)\n","    # Instantiate the various layers as an attribute of the Module for each layer\n","    self.fc = Linear(noise_dim, 128 * 7 * 7)\n","    self.leaky_relu_1 = LeakyReLU(0.2)\n","    self.convt_1 = ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1)\n","    self.leaky_relu_2 = LeakyReLU(0.2)\n","    self.convt_2 = ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1)\n","    self.leaky_relu_3 = LeakyReLU(0.2)\n","    self.conv = Conv2d(128, 1, kernel_size=7, padding=\"same\")\n","    self.sig = Sigmoid()\n","    # --- END CODE HERE \n","  \n","  def forward(self, x: Tensor) -> Tensor:\n","    # --- START CODE HERE (05)\n","    # Make the input x pass through all layers (do not forget the reshape)\n","\n","    x = self.fc(x)\n","    x = self.leaky_relu_1(x)\n","    x = x.view(-1, 128, 7, 7)\n","\n","    x = self.convt_1(x)\n","    x = self.leaky_relu_2(x)\n","\n","    x = self.convt_2(x)\n","    x = self.leaky_relu_3(x)\n","\n","    x = self.conv(x)\n","\n","    out = self.sig(x)\n","    # --- END CODE HERE \n","\n","    return out\n","\n","generator = Generator()\n","generator"]},{"cell_type":"markdown","metadata":{"id":"xu9ihScQT8py"},"source":["### Implementation of the Discriminator\n","\n","The Discriminator is a neural network that takes as input images of shape (1,28,28) and output a prediction of whether this image is real or fake.\n","\n","The architecture will be the following:\n","- a Convolutional layer of 64 filters of shape (3,3) and stride (2,2) and padding (1,1)\n","- a LeakyReLU with $\\alpha = 0.2$\n","- a Dropout layer of probability $0.4$\n","- a Convolutional layer of 64 filters of shape (3,3) and stride (2,2) and padding (1,1)\n","- a LeakyReLU with $\\alpha = 0.2$\n","- a Dropout layer of probability $0.4$\n","- a Flatten function\n","- a Fully-Connected layer of output size 1\n","- a Sigmoid activation layer"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"dSs9g7UiXlyg"},"outputs":[{"data":{"text/plain":["Discriminator(\n","  (conv_1): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","  (leaky_relu_1): LeakyReLU(negative_slope=0.2)\n","  (dropout_1): Dropout(p=0.4, inplace=False)\n","  (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","  (dropout_2): Dropout(p=0.4, inplace=False)\n","  (leaky_relu_2): LeakyReLU(negative_slope=0.2)\n","  (fc): Linear(in_features=3136, out_features=1, bias=True)\n","  (sig): Sigmoid()\n",")"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# --- START CODE HERE (06)\n","# Import the needed layers\n","from torch.nn import Dropout, Flatten\n","# --- END CODE HERE\n","\n","class Discriminator(Module):\n","  def __init__(self) -> None:\n","    super().__init__()\n","\n","    # --- START CODE HERE (07)\n","    # Instantiate the various layers as an attribute of the Module for each layer\n","    self.conv_1 = Conv2d(1, 64, kernel_size=3, stride=2, padding=1)\n","    self.leaky_relu_1 = LeakyReLU(0.2)\n","    self.dropout_1 = Dropout(0.4)\n","    self.conv_2 = Conv2d(64, 64, kernel_size=3, stride=2, padding=1)\n","    self.dropout_2 = Dropout(0.4)\n","    self.leaky_relu_2 = LeakyReLU(0.2)\n","    self.fc = Linear(3136, 1)\n","    self.sig = Sigmoid()\n","    # --- END CODE HERE \n","  \n","  def forward(self, x: Tensor) -> Tensor:\n","    batch_size = x.shape[0]\n","\n","    # --- START CODE HERE (08)\n","    # Make the input x pass through all layers (do not forget the reshape)\n","    x = self.conv_1(x)\n","    x = self.leaky_relu_1(x)\n","    x = self.dropout_1(x)\n","\n","    x = self.conv_2(x)\n","    x = self.leaky_relu_2(x)\n","    x = self.dropout_2(x)\n","\n","    x = x.view(batch_size, -1)\n","    x = self.fc(x)\n","\n","    out = self.sig(x)\n","    # --- END CODE HERE \n","\n","    out = out.squeeze(1) # squeeze the tensor for BCE loss.\n","\n","    return out\n","  \n","discriminator = Discriminator()\n","discriminator"]},{"cell_type":"markdown","metadata":{"id":"e2Sz-0AzeP6Q"},"source":["#### Training loop\n","\n","For each neural network, a different optimizer will be defined to be able to optimize parameters differently.\n","\n","We will have to make two steps of optimization per iteration: one for the generator and the other one for the discriminator.\n","\n","The optimization step is for the discriminator:\n","- Generate fake data\n","- Draw real data\n","- Pass the fake data and the real data to the discriminator\n","- Compute $L_D$\n","- Optimize the discriminator parameters\n","\n","The optimization step is for the generator:\n","- Generate fake data\n","- Pass the fake data to the discriminator\n","- Compute $L_G$\n","- Optimize the generator parameters\n","\n","As you can see, both forward pass requires to generator fake data. To optimize training and avoid unecessary computation, we will use for both the same generated data.\n","\n","However as soon as the Generator generates data, it is stored in a [computational graph](https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/) in the backend of Pytorch to optimize parameters. To avoid conflic of graph and source of errors, when passing the fake data to the discriminator you will first have to [detach](https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html) the fake data. "]},{"cell_type":"markdown","metadata":{"id":"bk2uu5ULjHPa"},"source":["##### Update discriminator parameters\n","\n","Implement the optimization step defined above. We pass as arguments what is required and some of theses arguments, such as the discriminator and optimizer will be defined later."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"WxO4emHsbI3u"},"outputs":[],"source":["from torch.optim import Optimizer\n","\n","def discriminator_optimizer_step(\n","  real_data: Tensor,\n","  fake_data: Tensor,\n","  discriminator: Module,\n","  optimizer: Optimizer,\n","  loss_fn: Module\n","):\n","  \"\"\"\n","  Perform a discriminator optimization step.\n","  \n","  Args:\n","    real_data: the real data.\n","    fake_data: the generated data that has been detached.\n","    discriminator: the discriminator model to perform the pass.\n","    optimizer: the optimizer to optimize the discriminator parameters.\n","    loss_fn: the loss to apply to the discriminator\n","  Returns:\n","    The loss value and the accuracy for real and for fake images of the discriminator.\n","  \"\"\"\n","\n","  batch_size = real_data.shape[0]\n","  device = real_data.device\n","\n","  discriminator.zero_grad()\n","\n","  # --- START CODE HERE (09)\n","  # Concatenate the real and fake data\n","  data = torch.cat((real_data, fake_data))\n","\n","  # Create the vector of label\n","  y = torch.cat((torch.ones(real_data.shape[0], device=device), torch.zeros(fake_data.shape[0], device=device)))\n","\n","  # Make predictions\n","  preds = discriminator(data)\n","  # --- END CODE HERE\n","\n","  loss = loss_fn(preds, y)\n","  accuracy_real = compute_accuracy(preds[:batch_size], y[:batch_size])\n","  accuracy_fake = compute_accuracy(preds[batch_size:], y[batch_size:])\n","\n","  loss.backward()\n","\n","  optimizer.step()\n","\n","  return loss.item(), accuracy_real, accuracy_fake # for metrics display"]},{"cell_type":"markdown","metadata":{"id":"w5NWaRV_njeS"},"source":["##### Update generator parameters\n","\n","Implement the generator step defined above. We pass as arguments what is required and some of theses arguments, such as the discriminator and optimizer will be defined later.\n","\n","We suppose that generator already generated the images that is used by the discriminator, that is why we do not pass through the generator but only through the discriminator to compute the loss."]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n","DEVICE = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"A62zL4UDnig9"},"outputs":[],"source":["def generator_optimizer_step(\n","  fake_data: Tensor,\n","  generator: Module,\n","  discriminator: Module,\n","  optimizer: Optimizer,\n","  loss_fn: Module\n",") -> float:\n","  \"\"\"\n","  Perform a discriminator optimization step.\n","  \n","  Args:\n","    fake_data: the generated data that has been detached.\n","    generator: the generator model that generated the data.\n","    discriminator: the discriminator model to compute the loss.\n","    optimizer: the optimizer to optimize the generator parameters.\n","    loss_fn: the loss to apply to the generator\n","  Returns:\n","    The loss value.\n","  \"\"\"\n","\n","  generator.zero_grad()\n","\n","  # --- START CODE HERE (10)\n","  # Create the vector of label\n","  y = torch.ones(fake_data.shape[0], device=DEVICE)\n","\n","  # Make predictions\n","  preds = discriminator(fake_data)\n","  # --- END CODE HERE\n","\n","  loss = loss_fn(preds, y)\n","\n","  loss.backward()\n","\n","  optimizer.step()\n","\n","  return loss.item() # for metrics display"]},{"cell_type":"markdown","metadata":{"id":"1q4yyqmy48zB"},"source":["#### Generate fake data\n","\n","To generate images, we need to sample random points from the random distribution. We will the [randn](https://pytorch.org/docs/stable/generated/torch.randn.html) function for pytorch that samples from a normal distribution for each feature of a data point."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"eHVn_b-o48IU"},"outputs":[{"data":{"text/plain":["torch.Size([10, 1, 28, 28])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["def generate_fake_data(n_samples: int, feature_dim: int, generator: Module, device: str = \"cpu\") -> Tensor:\n","  \"\"\"\n","  Generate n_samples fake data images.\n","\n","  Args:\n","    n_samples: the number of fake images to generate.\n","    feature_dim: the feature dim of the noise vector.\n","    generator: the generator module.\n","    device: the device to make data on.\n","  Returns:\n","    the fake images.\n","  \"\"\"\n","\n","  # --- START CODE HERE (10)\n","  # Sample the random noise and feed it to the generator.\n","  noise = torch.randn(n_samples, feature_dim, device=device)\n","  fake_data = generator(noise)\n","  # --- END CODE HERE\n","\n","  return fake_data\n","\n","generate_fake_data(10, 100, generator, DEVICE).shape # should output the shape (10, 1, 28, 28)"]},{"cell_type":"markdown","metadata":{"id":"3g7x2Yyr2H67"},"source":["#### Make the training loop\n","\n","In the training loop, we will iterate over the provided dataloader for several epochs and perform:\n","- Data Generation\n","- Update of the discrimanator parameters\n","- Update of the generator parameters\n","\n","We will define the loss function that is the cross-entropy for both the generator and the discrimator as detailled before and show various metrics.\n","\n","Aswell as displaying metrics, we will also display qualitative result which is the images generated throughout training."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"NNPTW_rU2vaF"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","from typing import Tuple, Any\n","import time\n","\n","# --- START CODE HERE (11)\n","# Import the loss function\n","from torch.nn import BCELoss\n","# --- END CODE HERE\n","\n","def training_loop_gan(\n","  dataloader: DataLoader, \n","  generator: Module, \n","  generator_optimizer: Optimizer,\n","  discriminator: Module, \n","  discriminator_optimizer: Optimizer,\n","  n_epochs: int = 20,\n","  noise_dim: int = 100,\n","  n_images_qualitative: int = 25,\n","  device: str = \"cpu\"\n",") -> Tuple[Any]:\n","  \"\"\"\n","  Perform the training_loop for the GAN model.\n","\n","  Args:\n","    dataloader: the dataloader over the training data.\n","    generator: the generator model.\n","    generator_optimizer: the optimizer to optimize the generator parameters.\n","    discriminator: the discriminator model.\n","    discriminator_optimizer: the optimizer to optimize the discriminator parameters.\n","    n_epochs: the number of epochs to perform.\n","    n_images_qualitative: the number of images to generate per epoch for qualitative results.\n","    device: the device to make computations on.\n","  \n","  Returns:\n","    Tuple of:\n","      - list of discriminator loss per epoch.\n","      - list of discriminator accuracy per epoch.\n","      - list of generator loss per epoch.\n","      - list of qualitative images generated per epoch.\n","  \"\"\"\n","\n","  # --- START CODE HERE (12)\n","  # Instantiate the loss function\n","  loss_fn = BCELoss().to(device)\n","  # --- END CODE HERE\n","\n","  discriminator_loss_epoch = []\n","  discriminator_accuracy_real_epoch, discriminator_accuracy_fake_epoch = [], []\n","  generator_loss_epoch = []\n","  qualitative_images_epoch = []\n","\n","  start_time = time.time()\n","\n","  # Random noise used for visualization at each epoch\n","  random_noise = torch.randn(n_images_qualitative, noise_dim, device=device) \n","\n","  for num_epoch in range(n_epochs):\n","    discriminator_loss_iter = []\n","    discriminator_accuracy_real_iter, discriminator_accuracy_fake_iter = [], []\n","    generator_loss_iter = []\n","\n","    generator.train()\n","    discriminator.train()\n","    for idx, batch in enumerate(dataloader):\n","      real_data, _ = batch\n","\n","      if device == \"cuda\":\n","        real_data = real_data.cuda(non_blocking=True)\n","\n","      n_samples = real_data.shape[0]\n","\n","\n","      # --- START CODE HERE (13)\n","      # Here you need to call in the right order and with the right parameters \n","      # the different functions defined before to optimize both the generator \n","      # and discriminator\n","\n","      fake_data = generate_fake_data(n_samples, noise_dim, generator, device)\n","\n","      fake_data_detached = fake_data.detach()\n","      \n","      discriminator_loss, discriminator_accuracy_real, discriminator_accuracy_fake =\\\n","       discriminator_optimizer_step(real_data, fake_data_detached, discriminator,\n","                                    discriminator_optimizer, loss_fn)\n","      generator_loss = generator_optimizer_step(fake_data, generator, discriminator, generator_optimizer, loss_fn)\n","\n","      # --- END CODE HERE\n","\n","      discriminator_loss_iter.append(discriminator_loss)\n","      discriminator_accuracy_real_iter.append(discriminator_accuracy_real)\n","      discriminator_accuracy_fake_iter.append(discriminator_accuracy_fake)\n","      generator_loss_iter.append(generator_loss)\n","    \n","    discriminator_loss_epoch.append(torch.mean(torch.tensor(discriminator_loss_iter)))\n","    discriminator_accuracy_real_epoch.append(torch.mean(torch.tensor(discriminator_accuracy_real_iter)))\n","    discriminator_accuracy_fake_epoch.append(torch.mean(torch.tensor(discriminator_accuracy_fake_iter)))\n","    generator_loss_epoch.append(torch.mean(torch.tensor(generator_loss_iter)))\n","    \n","    # Image generator for visualization at each epoch\n","    with torch.no_grad():\n","      generator.eval()\n","      qualitative_images = generator(random_noise)\n","      qualitative_images_epoch.append(qualitative_images)\n","\n","    print(\"epoch: {0:d} in {1:04d} seconds (loss: discriminator {2:.4f} generator {3:.4f}) (accuracy: real {4:.4f} fake {5:.4f})\".format(num_epoch, int(time.time()-start_time), discriminator_loss_epoch[-1], generator_loss_epoch[-1], discriminator_accuracy_real_epoch[-1], discriminator_accuracy_fake_epoch[-1]))\n","  \n","  return discriminator_loss_epoch, discriminator_accuracy_real_epoch, discriminator_accuracy_fake_epoch, generator_loss_epoch, qualitative_images_epoch\n"]},{"cell_type":"markdown","metadata":{"id":"dko1iGhImDn3"},"source":["#### Perform the training of the model\n","\n","Now that everything is implemented, we can actually perform the training of our GAN.\n","\n","We need to instantiate the various models on the right device and to [apply](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.apply) the initialization of their weights.\n","\n","We will use the optimizer ADAM for both networks with the following parameters:\n","- $lr = 0.0002$\n","- $\\beta_1 = 0.5$\n","- $\\beta_2 = 0.999$ "]},{"cell_type":"code","execution_count":16,"metadata":{"id":"WDSS2Rig9wCA"},"outputs":[{"name":"stdout","output_type":"stream","text":["Random Seed:  4017\n","epoch: 0 in 0668 seconds (loss: discriminator 0.6681 generator 0.8029) (accuracy: real 0.5867 fake 0.5677)\n","epoch: 1 in 1383 seconds (loss: discriminator 0.6899 generator 0.7108) (accuracy: real 0.5403 fake 0.5157)\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [16], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m discriminator_optimizer \u001b[38;5;241m=\u001b[39m Adam(discriminator\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0002\u001b[39m, betas\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.999\u001b[39m))\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# --- END CODE HERE\u001b[39;00m\n\u001b[1;32m     33\u001b[0m discriminator_loss_epoch, discriminator_accuracy_real_epoch, discriminator_accuracy_fake_epoch, generator_loss_epoch, qualitative_gan_images_epoch \u001b[38;5;241m=\u001b[39m\\\n\u001b[0;32m---> 34\u001b[0m   training_loop_gan(dataloader, generator, generator_optimizer, discriminator, discriminator_optimizer, n_epochs, noise_dim, n_images_qualitative, device)\n","Cell \u001b[0;32mIn [15], line 85\u001b[0m, in \u001b[0;36mtraining_loop_gan\u001b[0;34m(dataloader, generator, generator_optimizer, discriminator, discriminator_optimizer, n_epochs, noise_dim, n_images_qualitative, device)\u001b[0m\n\u001b[1;32m     80\u001b[0m fake_data_detached \u001b[38;5;241m=\u001b[39m fake_data\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     82\u001b[0m discriminator_loss, discriminator_accuracy_real, discriminator_accuracy_fake \u001b[38;5;241m=\u001b[39m\\\n\u001b[1;32m     83\u001b[0m  discriminator_optimizer_step(real_data, fake_data_detached, discriminator,\n\u001b[1;32m     84\u001b[0m                               discriminator_optimizer, loss_fn)\n\u001b[0;32m---> 85\u001b[0m generator_loss \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# --- END CODE HERE\u001b[39;00m\n\u001b[1;32m     89\u001b[0m discriminator_loss_iter\u001b[38;5;241m.\u001b[39mappend(discriminator_loss)\n","Cell \u001b[0;32mIn [13], line 33\u001b[0m, in \u001b[0;36mgenerator_optimizer_step\u001b[0;34m(fake_data, generator, discriminator, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# --- END CODE HERE\u001b[39;00m\n\u001b[1;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(preds, y)\n\u001b[0;32m---> 33\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/machine_learning/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n","File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/machine_learning/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch.backends.cudnn as cudnn\n","import random\n","\n","from torch.optim import Adam\n","cudnn.benchmark = True\n","\n","#set manual seed to a constant get a consistent output\n","manualSeed = 4017\n","print(\"Random Seed: \", manualSeed)\n","random.seed(manualSeed)\n","torch.manual_seed(manualSeed)\n","\n","\n","n_epochs = 25\n","noise_dim = 100\n","n_images_qualitative = 25\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","dataloader = DataLoader(dataset_train, shuffle=True, batch_size=128, drop_last=True, num_workers=2)\n","\n","# --- START CODE HERE (14)\n","# Instantiate the models and optimizers\n","generator = Generator().to(device)\n","generator.apply(weights_init)\n","\n","discriminator = Discriminator().to(device)\n","discriminator.apply(weights_init)\n","\n","generator_optimizer = Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","discriminator_optimizer = Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","# --- END CODE HERE\n","\n","discriminator_loss_epoch, discriminator_accuracy_real_epoch, discriminator_accuracy_fake_epoch, generator_loss_epoch, qualitative_gan_images_epoch =\\\n","  training_loop_gan(dataloader, generator, generator_optimizer, discriminator, discriminator_optimizer, n_epochs, noise_dim, n_images_qualitative, device)\n"]},{"cell_type":"markdown","metadata":{"id":"qtVCmCbcneyX"},"source":["#### Visualize your results"]},{"cell_type":"markdown","metadata":{"id":"OqtBHtLjorvC"},"source":["##### **Loss and accuracies**\n","\n","You should observe that the discriminator is not accurate over training and that the Generator keeps fooling it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i9Q7bwVskclI"},"outputs":[],"source":["display_losses_and_accuracies_gan(discriminator_loss_epoch, generator_loss_epoch, discriminator_accuracy_real_epoch, discriminator_accuracy_fake_epoch)"]},{"cell_type":"markdown","metadata":{"id":"j7ZZIkUznvtc"},"source":["##### **Images generated**\n","\n","This is the qualitative result of our GAN throughout training. \n","\n","We have to subsample the number of grids to show in comparison with what was generated to make it work in Colab.\n","\n","Still we get the general idea that our Generator is getting quite good at forming new images. \n","\n","**You can train for more epochs, try different noise dimension, to reach better generation !**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LLbR0UEwNsPS"},"outputs":[],"source":["anim = get_slide_show_fake_images_in_grid(qualitative_gan_images_epoch, every_n_grid=5)\n","anim"]},{"cell_type":"markdown","metadata":{"id":"xdSVQoOyq4HG"},"source":["## Bonus: Variational Autoencoder (VAE)"]},{"cell_type":"markdown","metadata":{"id":"MF-jWCRw2IL9"},"source":["### Introduction\n","\n","VAEs have been first proposed in 2014 by [Ian Goodfellow et al]()\n","\n","They are autoencoders that assume the data comes from a specific type of distribution (usually Gaussian) and attempts to find this distribution.\n","\n","A VAE is composed of two different Neural Networks and during training they do the following:\n","- The encoder learns to encode the image $x$ to retrieve the data distribution $p(z|x)$ of the latent representation $z$ following a prior distribution $q(z|x) \\sim \\mathcal{N}(0,1)$.\n","- The decoder learns to generate image $\\hat{x}$ based on noise drawn from the encoder proposed data distribution. \n","\n","At inference, we will sample noise from the prior distribution, pass it through the decoder to retrieve generated images.\n","\n","![](https://miro.medium.com/max/4800/1*qtXrzMLorYDl4SzKqoZxBg.png)"]},{"cell_type":"markdown","metadata":{"id":"dHhuzFpe4NwW"},"source":["### Train the VAE\n","\n","To train the VAE model, we will minimize the [KL divergence](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html) between the prior and the estimated distribution:\n","\n","$$L_{KL} = \\mathcal{KL}\\left( q(z|x) \\parallel p(z|x) \\right) = \\mathcal{KL}\\left( \\mathcal{N}(\\hat{\\mu}, \\hat{\\sigma}) \\parallel \\mathcal{N}(0, 1) \\right) = \\sum_{x \\in X} \\left( \\hat{\\sigma}^2 + \\hat{\\mu}^2 - \\log \\hat{\\sigma} - \\frac{1}{2} \\right)$$\n","\n","We will also minimize the reconstruction loss between the reconstructed image and the image:\n","\n","$$L_{recon} = \\parallel \\hat{x} - x ∥^1$$\n","\n","The $L_{recon}$ loss will be implemented using the [binary cross entropy](https://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy.html).\n","\n","\n","The final loss is:\n","$$L_{VAE} = L_{recon} + \\beta L_{KL}$$"]},{"cell_type":"markdown","metadata":{"id":"fx41PaAHiQid"},"source":["#### Import the used layers and classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NPlAzlksf6n_"},"outputs":[],"source":["from torch import Tensor\n","from torch.nn import Conv2d, ConvTranspose2d, Dropout, ReLU, Linear, Module, Sigmoid\n","from typing import Tuple\n","import torch.nn.functional as F"]},{"cell_type":"markdown","metadata":{"id":"MCDUkrebiVQd"},"source":["#### Implementation of the Encoder\n","\n","The Encoder is a neural network that takes as input images of shape (1,28,28) and output $\\log(\\hat{\\sigma})$ and $\\hat{\\mu}$ that parametrises the $p(z|x)$.\n","\n","The architecture will be the following:\n","- a Convolutional layer of 32 filters of shape (3,3) and stride (2,2)\n","- a ReLU layer\n","- a Convolutional layer of 64 filters of shape (3,3) and stride (2,2)\n","- a ReLU layer\n","- a Flatten function\n","- a Fully-Connected layer of output size $noise\\_dim * 2$\n","\n","The output is then splitted in half according to the second dimension to output $\\log(\\hat{\\sigma})$ and $\\hat{\\mu}$.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7MmwyYz0krH2"},"outputs":[],"source":["class Encoder(Module):\n","  def __init__(self, noise_dim: int = 1) -> None:\n","    super().__init__()\n","\n","    self.noise_dim = noise_dim\n","\n","    # --- START CODE HERE (15)\n","    # Instantiate the various layers as an attribute of the Module for each layer\n","    self.conv_1 =\n","    self.relu_1 =\n","    self.conv_2 =\n","    self.relu_2 =\n","    self.fc =\n","    # --- END CODE HERE \n","  \n","  def forward(self, x: Tensor) -> Tensor:\n","    batch_size = x.shape[0]\n","\n","    # --- START CODE HERE (16)\n","    # Make the input x pass through all layers (do not forget the reshape)\n","\n","\n","    out =\n","    # --- END CODE HERE\n","\n","    mu, log_sigma = out[:, :self.noise_dim], out[:, self.noise_dim:]\n","\n","    return mu, log_sigma\n","  \n","encoder = Encoder()\n","encoder"]},{"cell_type":"markdown","metadata":{"id":"RsZLrWmwijO_"},"source":["#### Implementation of the Decoder\n","\n","The Decoder is a neural network that takes as an input a random noise of size $noise\\_dim$. We will increase its dimensionality, reshape it according to a grid and use several convolutional transposed layers to upsample its resolution to reach the size of the MNIST dataset and generate an image thanks to the sigmoid activation function.\n","\n","The architecture will be the following:\n","- a Fully-Connected layer of output size $32 \\times 7 \\times 7$\n","- a ReLU layer\n","- a Reshape is applied to arrange data according to a grid of shape (32, 7, 7)\n","- a Convolutional Transposed layer of 64 filters of shape (4,4) and stride (2,2) and padding (1,1)\n","- a ReLU layer\n","- a Convolutional Transposed layer of 32 filters of shape (4,4) and stride (2,2) and padding (1,1)\n","- a LeakyReLU layer with $\\alpha = 0.2$\n","- a Convolutional layer of 1 filter of shape (3,3) and same padding\n","- a Sigmoid activation layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7vhZEjXWat-H"},"outputs":[],"source":["class Decoder(Module):\n","  def __init__(self, noise_dim: int = 1) -> None:\n","    \"\"\"\n","    Create a decoder.\n","\n","    Args:\n","      noise_dim: the dimension of the input noise.\n","    \"\"\"\n","\n","    super().__init__()\n","\n","    # --- START CODE HERE (17)\n","    # Instantiate the various layers as an attribute of the Module for each layer\n","    self.fc =\n","    self.relu_1 =\n","    self.convt_1 =\n","    self.relu_2 =\n","    self.convt_2 =\n","    self.relu_3 =\n","    self.conv =\n","    self.sig =\n","    # --- END CODE HERE \n","  \n","  def forward(self, x: Tensor) -> Tensor:\n","    # --- START CODE HERE (18)\n","    # Make the input x pass through all layers (do not forget the reshape)\n","    \n","    out = self.sig(x)\n","    # --- END CODE HERE \n","\n","    return out\n","\n","decoder = Decoder()\n","decoder"]},{"cell_type":"markdown","metadata":{"id":"42b25mInkHU5"},"source":["#### Implementation of the VAE model\n","\n","We will define a VAE wrapper that takes as an input both the encoder and the decoder.\n","\n","The input of the VAE model will be an image and the pipeline is:\n","- Pass the image through the encoder to retrieve $\\log(\\hat{\\sigma})$ and $\\hat{\\mu}$.\n","- Sample $z \\sim \\mathcal{N}(\\hat{\\mu}, \\hat{\\sigma})$.\n","- Pass $z$ through the decoder.\n","- Return the losses."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0xZcV2zCgEoE"},"outputs":[],"source":["class VAE(Module):\n","  def __init__(self, encoder: Module, decoder: Module) -> None:\n","    \"\"\"\n","    Create a VAE.\n","\n","    Args:\n","      encoder: the encoder.\n","      decoder: the decoder.\n","    \"\"\"\n","    super().__init__()\n","\n","    self.encoder = encoder\n","    self.decoder = decoder\n","  \n","  def sampling(self, mu, log_sigma):\n","      std = torch.exp(0.5 * log_sigma)\n","      eps = torch.randn_like(std)\n","      return eps.mul(std).add_(mu)\n","  \n","  def loss_kl(self, mu, log_sigma):\n","    # --- START CODE HERE (19)\n","    # Write the kl loss.\n","    kl_loss =\n","    # --- END CODE HERE \n","    return kl_loss\n","  \n","  def loss_reconstruction(self, recon_x, x):\n","    # --- START CODE HERE (20)\n","    # Write the reconstruction loss.\n","    recon_loss =\n","    # --- END CODE HERE \n","    return recon_loss\n","\n","  def forward(self, x: Tensor) -> Tuple[Tensor]:\n","    # --- START CODE HERE (21)\n","    # Implement the forward pass for the VAE by using the encoder, decoder, and the functions define above.\n","    mu, log_sigma =\n","    z =\n","    recon_x =\n","    kl_loss =\n","    recon_loss =\n","    # --- END CODE HERE \n","\n","    return kl_loss, recon_loss\n","  \n","  def generate_image(self, x: Tensor) -> Tensor:\n","    return self.decoder(x)\n","\n","vae = VAE(encoder, decoder)\n","vae"]},{"cell_type":"markdown","metadata":{"id":"aZ5vXrEZqYir"},"source":["#### Make the training loop\n","\n","In the training loop, we will iterate over the provided dataloader for several epochs and perform:\n","- Update of the VAE parameters\n","- Data Generation\n","\n","We will add the losses outputed by the VAE model to compute:\n","$$L_{VAE} = L_{recon} + \\beta L_{KL}$$\n","\n","Aswell as displaying metrics, we will also display qualitative result which is the images generated throughout training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TPNNstrJiLP9"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","from torch.optim import Optimizer\n","from typing import Tuple, Any\n","import time\n","\n","\n","def training_loop_vae(\n","  dataloader: DataLoader, \n","  vae: Module, \n","  vae_optimizer: Optimizer,\n","  beta: float = 1.,\n","  n_epochs: int = 20,\n","  noise_dim: int = 100,\n","  n_images_qualitative: int = 25,\n","  device: str = \"cpu\"\n",") -> Tuple[Any]:\n","  \"\"\"\n","  Perform the training_loop for the GAN model.\n","\n","  Args:\n","    dataloader: the dataloader over the training data.\n","    vae: the vae model.\n","    vae_optimizer: the optimizer to optimize the vae parameters.\n","    beta: the loss coefficient for the KL divergence.\n","    n_epochs: the number of epochs to perform.\n","    n_images_qualitative: the number of images to generate per epoch for qualitative results.\n","    device: the device to make computations on.\n","  \n","  Returns:\n","    Tuple of:\n","      - list of kl vae loss per epoch.\n","      - list of reconstruction vae loss per epoch.\n","      - list of qualitative images generated per epoch.\n","  \"\"\"\n","\n","  kl_loss_epoch = []\n","  recon_loss_epoch = []\n","  qualitative_images_epoch = []\n","\n","  start_time = time.time()\n","\n","  # Random noise used for visualization at each epoch\n","  random_noise = torch.randn(n_images_qualitative, noise_dim, device=device) \n","\n","  for num_epoch in range(n_epochs):\n","    kl_loss_iter = []\n","    recon_loss_iter = []\n","\n","    vae.train()\n","    for idx, batch in enumerate(dataloader):\n","      real_data, _ = batch\n","\n","      if device == \"cuda\":\n","        real_data = real_data.cuda(non_blocking=True)\n","\n","      n_samples = real_data.shape[0]\n","\n","      vae_optimizer.zero_grad()\n","\n","      # --- START CODE HERE (22)\n","      # Here you need to make the optimization step of your vae.\n","      # The loss is defined by the sum of the kl multiplied by beta and\n","      # the reconstruction\n","\n","      kl_loss, recon_loss =\n","\n","      loss =\n","\n","      # --- END CODE HERE\n","\n","      loss.backward()\n","      vae_optimizer.step()\n","\n","      kl_loss_iter.append(kl_loss.item())\n","      recon_loss_iter.append(recon_loss.item())\n","    \n","    kl_loss_epoch.append(torch.mean(torch.tensor(kl_loss_iter)))\n","    recon_loss_epoch.append(torch.mean(torch.tensor(recon_loss_iter)))\n","    \n","    # Image generator for visualization at each epoch\n","    with torch.no_grad():\n","      vae.eval()\n","      qualitative_images = vae.generate_image(random_noise)\n","      qualitative_images_epoch.append(qualitative_images)\n","\n","    print(\"epoch: {0:d} in {1:04d} seconds (loss: kl {2:.4f} recon {3:.4f})\".format(num_epoch, int(time.time()-start_time), kl_loss_epoch[-1], recon_loss_epoch[-1]))\n","  \n","  return kl_loss_epoch, recon_loss_epoch, qualitative_images_epoch\n"]},{"cell_type":"markdown","metadata":{"id":"dSh90I5nqRRa"},"source":["#### Perform the training of the model\n","\n","Now that everything is implemented, we can actually perform the training of our GAN.\n","\n","We need to instantiate the various models on the right device and to [apply](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.apply) the initialization of their weights.\n","\n","We will use the optimizer ADAM for both networks with the following parameters:\n","- $lr = 0.0001$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rT_FW2YCmSK6"},"outputs":[],"source":["import torch.backends.cudnn as cudnn\n","import random\n","\n","from torch.optim import Adam\n","cudnn.benchmark = True\n","\n","#set manual seed to a constant get a consistent output\n","manualSeed = 4017\n","print(\"Random Seed: \", manualSeed)\n","random.seed(manualSeed)\n","torch.manual_seed(manualSeed)\n","\n","#Define the hyper-parameters to train.\n","n_epochs = 25\n","noise_dim = 2\n","beta = 1.\n","n_images_qualitative = 25\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","dataloader = DataLoader(dataset_train, shuffle=True, batch_size=128, drop_last=True, num_workers=2)\n","\n","# --- START CODE HERE (23)\n","# Instantiate the models and optimizers\n","encoder =\n","decoder =\n","vae =\n","vae =\n","vae.\n","\n","vae_optimizer =\n","# --- END CODE HERE\n","\n","kl_loss_epoch, recon_loss_epoch, qualitative_vae_images_epoch =\\\n","  training_loop_vae(dataloader, vae, vae_optimizer, beta, n_epochs, noise_dim, n_images_qualitative, device)\n"]},{"cell_type":"markdown","metadata":{"id":"hEleY-zTlvJ0"},"source":["#### Visualize your results"]},{"cell_type":"markdown","metadata":{"id":"_ZUVul1Tlzq8"},"source":["##### **Losses**\n","\n","You should observe that the reconstruction loss diminuish. The KL loss depending on the $\\beta$ coefficient have different behavior and can either:\n","- decrease.\n","- increase.\n","- decrease then increase.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C4pYTVm5mqyL"},"outputs":[],"source":["display_losses_and_accuracies_vae(kl_loss_epoch, recon_loss_epoch)"]},{"cell_type":"markdown","metadata":{"id":"iMmUl0o_mM2Q"},"source":["##### **Images generated**\n","\n","This is the qualitative result of our VAE throughout training. \n","\n","We have to subsample the number of grids to show in comparison with what was generated to make it work in Colab.\n","\n","Still we get the general idea that our VAE is getting quite good at forming new images. \n","\n","**You can train for more epochs, try different $\\beta$ coefficients and dimension for the noise, to reach better generation !**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gWUFl5z6ivrc"},"outputs":[],"source":["anim = get_slide_show_fake_images_in_grid(qualitative_vae_images_epoch, every_n_grid=5)\n","anim"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["IoAPIJbrTmi4","UcVlB74JTq22","0iO6w8GcTxr3"],"provenance":[]},"kernelspec":{"display_name":"Python 3.9.13 ('machine_learning')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"d38c4cc43721ac0d114dc8b1e646949e6883c7f0d964ef46ebccf5a0ec111b05"}}},"nbformat":4,"nbformat_minor":0}
